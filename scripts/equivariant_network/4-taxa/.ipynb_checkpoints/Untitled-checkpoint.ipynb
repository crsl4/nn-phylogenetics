{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b555a74b-0c6f-4611-8ffb-d65efe03a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# we fix the random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import itertools\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from os import path\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278375f2-bda4-4284-94a0-8c5111386f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# function to convert string to numbers\n",
    "def convert_string_to_numbers(str, dict):\n",
    "    ''' str: is the string to convert,\n",
    "        dict: dictionary with the relative ordering of each char'''\n",
    "\n",
    "    # create a map iterator using a lambda function\n",
    "    numbers = map(lambda x: dict[x], str)\n",
    "\n",
    "    return np.fromiter(numbers, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6417bc0-6fa4-468b-90af-d9e456a321ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetModule(torch.nn.Module):\n",
    "    '''Dense Residual network acting on each site, thus\n",
    "    implemtented via a Conv1 with window size equals to one\n",
    "    '''\n",
    "\n",
    "    def __init__(self, channel_count):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(channel_count, channel_count, 1),\n",
    "            torch.nn.BatchNorm1d(channel_count),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv1d(channel_count, channel_count, 1),\n",
    "            torch.nn.BatchNorm1d(channel_count),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e27b302-10de-44b1-a22a-d129af017fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescriptorModule(torch.nn.Module):\n",
    "    ''' Class implementing the Descriptor module, in this case we implement\n",
    "    in a unified manner, D_I, D_{II}, and D_{III}.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, length_dict, embedding_dim, trunc_length = 1550):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_layer = nn.Embedding(length_dict, embedding_dim)\n",
    "        self._res_module_1 = ResNetModule(embedding_dim)\n",
    "        self._res_module_2 = ResNetModule(embedding_dim)\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (none, 4, 1550)\n",
    "\n",
    "        # we use an embedding layer first\n",
    "        x = self.embedding_layer(x).permute([0, 1, 3, 2])\n",
    "        # (none, 4, 1550, chn_dim) without permute\n",
    "        # (none, 4, chn_dim, 1550) with permutation\n",
    "\n",
    "        # we apply \\phi\n",
    "        d0 =  self._res_module_1(x[:,0,:,:])\n",
    "        d1 =  self._res_module_1(x[:,1,:,:])\n",
    "        d2 =  self._res_module_1(x[:,2,:,:])\n",
    "        d3 =  self._res_module_1(x[:,3,:,:])\n",
    "\n",
    "        # Quartet 1 (12|34)\n",
    "        # d01 = d0 + d1\n",
    "        d01 = self._res_module_2(d0 + d1)\n",
    "\n",
    "        # d23 = d2 + d3\n",
    "        d23 = self._res_module_2(d2 + d3)\n",
    "\n",
    "        # the first descriptor\n",
    "        D_1 = d01 + d23\n",
    "\n",
    "        #Quartet 2 (13|24)\n",
    "        # d02 = d0 + d2\n",
    "        d02 = self._res_module_2(d0 + d2)\n",
    "\n",
    "        # d13 = d1 + d3\n",
    "        d13 = self._res_module_2(d1 + d3)\n",
    "\n",
    "        # the second descriptor\n",
    "        D_2 = d02 + d13\n",
    "\n",
    "        # Quartet 3 (14|23)\n",
    "        # d03 = d0 + d3\n",
    "        d03 = self._res_module_2(d0 + d3)\n",
    "\n",
    "        # d12 = d1 + d2\n",
    "        d12 = self._res_module_2(d1 + d2)\n",
    "\n",
    "        # the third descriptor\n",
    "        D_3 = d03 + d12\n",
    "\n",
    "        x = torch.cat([torch.unsqueeze(D_1,1),\n",
    "                       torch.unsqueeze(D_2,1),\n",
    "                       torch.unsqueeze(D_3,1)], dim = 1)\n",
    "        # (none, 3, embedding_dim, 1550)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d24c19d-ef00-45a9-9396-f0234723bcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Model(torch.nn.Module):\n",
    "    \"\"\"A neural network model to predict phylogenetic trees.\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim = 80, hidden_dim = 20,\n",
    "                      num_layers = 3, output_size = 20,\n",
    "                      dropout = 0.0):\n",
    "        \"\"\"Create a neural network model.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.descriptor_model = DescriptorModule(20, embedding_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_size = output_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # we define the required elements for \\Psi\n",
    "        self.classifier = torch.nn.Linear(self.output_size, 1)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim,\n",
    "                           num_layers, dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, self.output_size)\n",
    "\n",
    "        # flatenning the parameters (required for the lstm)\n",
    "        self.rnn.flatten_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Function that infers the phylogenetic trees for the input sequences.\n",
    "        Input: x the raw sequences\n",
    "        Output: the scores for each topology\n",
    "        \"\"\"\n",
    "        # extracting the device and the batch size\n",
    "        device = x.device\n",
    "        batch_size = x.size()[0]\n",
    "\n",
    "        # this is the structure preserving embedding\n",
    "        g =  self.descriptor_model(x)\n",
    "\n",
    "        # we reshape the output tensor\n",
    "        X =  g.view(3*batch_size, self.embedding_dim, -1)\n",
    "\n",
    "        # (none*3, 1550, hidden_dim)\n",
    "        r_output, hidden = self.rnn(X.permute([0, 2, 1]))\n",
    "\n",
    "        # extracting only the last in the sequence\n",
    "        # (none*3, hidden_dim)\n",
    "        r_output_last = r_output[:, -1, :]\n",
    "\n",
    "        # not sure if this helps\n",
    "        out = r_output_last.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        # (none*3, out_put_dimensions)\n",
    "        output = self.fc(out)\n",
    "\n",
    "        X_combined = self.classifier(output)\n",
    "        # (3*none, 1)\n",
    "\n",
    "        X_combined = X_combined.view(batch_size, 3)\n",
    "\n",
    "        return X_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020178f7-ffd1-4413-a311-96ed6d1d2690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get name of the script\n",
    "# nameScript = sys.argv[0].split('/')[-1]\n",
    "nameScript = \"4-taxa.py\"\n",
    "# get json file name of the script\n",
    "nameJson = \"4-taxa.json\"\n",
    "print(\"=================================================\")\n",
    "print(\"Executing \" + nameScript + \" following \" + nameJson, flush = True)\n",
    "print(\"=================================================\")\n",
    "# opening Json file \n",
    "jsonFile = open(nameJson) \n",
    "dataJson = json.load(jsonFile)\n",
    "\n",
    "# loading the input data from the json file\n",
    "ngpu = dataJson[\"ngpu\"]                  # number of GPUS\n",
    "lr = dataJson[\"lr\"]                      # learning rate\n",
    "batch_size = dataJson[\"batchSize\"]       # batch size\n",
    "\n",
    "data_root = dataJson[\"dataRoot\"]         # data folder\n",
    "model_root = dataJson[\"modelRoot\"]       # folder to save the data\n",
    "\n",
    "label_files = dataJson[\"labelFile\"]      # file with labels\n",
    "sequence_files = dataJson[\"matFile\"]     # file with sequences\n",
    "\n",
    "n_epochs = dataJson[\"nEpochs\"]           # number of epochs\n",
    "\n",
    "# checking if the summary file exist in the json file\n",
    "# we will save several statistics in this file\n",
    "if \"summaryFile\" in dataJson:\n",
    "    summary_file = dataJson[\"summaryFile\"] \n",
    "else :\n",
    "    summary_file = \"summary_file.txt\"\n",
    "\n",
    "print(\"=================================================\\n\")\n",
    "print(\"Learning Rate {} \".format(lr))\n",
    "print(\"Batch Size {} \\n\".format(batch_size))\n",
    "print(\"=================================================\")\n",
    "\n",
    "label_char = []\n",
    "seq_string = []\n",
    "# we read the labels as list of strings\n",
    "for i in range(len(label_files)):\n",
    "    with open(data_root+label_files[i], 'r') as f:\n",
    "        labels = f.readlines()\n",
    "    \n",
    "    with open(data_root+sequence_files[i], 'r') as f:\n",
    "        seq = f.readlines()\n",
    "\n",
    "    \n",
    "    label_char += labels[0:10000]\n",
    "    seq_string += seq[0:40000]\n",
    "\n",
    "n_samples = len(label_char)\n",
    "seq_length = len(seq_string[0])-1\n",
    "\n",
    "# first we need to extract all the different chars\n",
    "strL = \"\"\n",
    "for c in seq_string[0][:-1]:\n",
    "    if not c in strL:\n",
    "        strL += c\n",
    "\n",
    "# we sort them\n",
    "strL = sorted(strL)\n",
    "\n",
    "# we give them a relative order\n",
    "dict_amino = {}\n",
    "for ii, c in enumerate(strL):\n",
    "    dict_amino[c] = ii\n",
    "\n",
    "# looping over the labels and create array. Here each element of the \n",
    "# label_char has the form \"1\\n\", so we only take the first one\n",
    "labels = np.fromiter(map(lambda x: int(x[0])-1,\n",
    "                         label_char), dtype= np.int64)\n",
    "\n",
    "mats = np.zeros((len(seq_string), seq_length), dtype = np.int64)\n",
    "\n",
    "# this is pretty slow (optimize in numba)\n",
    "for ii, seq in enumerate(seq_string):\n",
    "    # note each line has a \\n character at the end so we remove it\n",
    "    mats[ii,:] = convert_string_to_numbers(seq[:-1],\\\n",
    "                 dict_amino).reshape((1,seq_length))\n",
    "mats = mats.reshape((n_samples, -1, seq_length))    \n",
    "# dims of mats is (N_samples, n_sequences, seq_length)\n",
    "\n",
    "# we need to assign the truncation lenght\n",
    "trunc_length = 1550\n",
    "\n",
    "print(\"Total number of samples: {}\".format(labels.shape[0]))\n",
    "\n",
    "# specify loss function\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# define the model\n",
    "model = _Model(dropout = 0.2).to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"number of parameters is %d\"%count_parameters(model))\n",
    "\n",
    "# specify loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# specify scheduler\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=10, gamma=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe159bc-2470-455a-93ac-5971a8ae9142",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Training Loop\")\n",
    "\n",
    "# Set the training and the testing set\n",
    "train_indexes = np.array([np.arange(0,9000), np.arange(10000,19000), np.arange(20000,29000)])\n",
    "train_extractions_seq = mats[train_indexes]\n",
    "train_extractions_lab = labels[train_indexes]\n",
    "inputTrain = torch.from_numpy(np.concatenate((train_extractions_seq[0], train_extractions_seq[1], train_extractions_seq[2]), axis=0))\n",
    "outputTrain = torch.from_numpy(np.concatenate((train_extractions_lab[0], train_extractions_lab[1], train_extractions_lab[2]), axis=0))\n",
    "\n",
    "test_indexes = np.array([np.arange(9000,10000), np.arange(19000,20000), np.arange(29000,30000)])\n",
    "test_extractions_seq = mats[test_indexes]\n",
    "test_extractions_lab = labels[test_indexes]\n",
    "inputTest = torch.from_numpy(np.concatenate((test_extractions_seq[0], test_extractions_seq[1], test_extractions_seq[2]), axis=0))\n",
    "outputTest = torch.from_numpy(np.concatenate((test_extractions_lab[0], test_extractions_lab[1], test_extractions_lab[2]), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4dca30-b7a3-4010-81bd-c7cd5d3b61bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_acc (dataloaderTest, epoch, max_accuracy):\n",
    "    test_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    # we measure the time it takes to evaluat the test examples\n",
    "    start = time.time()\n",
    "\n",
    "    for genes, quartets_batch in dataloaderTest:\n",
    "\n",
    "        #send to the device (either cpu or gpu)\n",
    "        genes, quartets_batch = genes.to(device), quartets_batch.to(device)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "\n",
    "        quartetsNN = model(genes)\n",
    "        # calculate the loss\n",
    "        _, predicted = torch.max(quartetsNN, 1)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(quartetsNN, quartets_batch)\n",
    "\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        test_loss += loss.item()\n",
    "        total += quartets_batch.size(0)\n",
    "        correct += (predicted == quartets_batch).sum().item()\n",
    "\n",
    "    end = time.time()\n",
    "    accuracy_test = correct/total\n",
    "    print('Epoch: {} \\tTest accuracy: {:.6f}  \\tTime Elapsed: {:.6f}[s]'.format(epoch,\n",
    "                                                         accuracy_test,\n",
    "                                                         end - start))\n",
    "    if accuracy_test > max_accuracy:\n",
    "        max_accuracy = accuracy_test\n",
    "\n",
    "    return test_loss, accuracy_test, max_accuracy\n",
    "\n",
    "max_train_accuracy = 0\n",
    "get_epoch = []\n",
    "get_test1_acc, get_test2_acc, get_test3_acc = [],[],[]\n",
    "trainEpoch, trainLoss = [], []\n",
    "test_epoch = []\n",
    "get_test1_loss, get_test2_loss, get_test3_loss = [],[],[]\n",
    "max_accuracy_1, max_accuracy_2, max_accuracy_3 = 0,0,0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # monitor the time each epoch takes\n",
    "    start = time.time()\n",
    "\n",
    "    model.train()\n",
    "    train_total, train_correct = 0, 0\n",
    "\n",
    "    # load the dataloader\n",
    "    datasetTrain = data.TensorDataset(inputTrain, outputTrain)\n",
    "    dataloaderTrain = torch.utils.data.DataLoader(datasetTrain,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=True,\n",
    "                                                  pin_memory=True)\n",
    "\n",
    "    # Train the model\n",
    "    for genes, quartets_batch in dataloaderTrain:\n",
    "\n",
    "        # send to the device (either cpu or gpu)\n",
    "        genes, quartets_batch = genes.to(device), quartets_batch.to(device)\n",
    "\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        quartetsNN = model(genes)\n",
    "        _, predicted = torch.max(quartetsNN, 1)\n",
    "\n",
    "        #calculate training accuracy\n",
    "        train_total += quartets_batch.size(0)\n",
    "        train_correct += (predicted == quartets_batch).sum().item()\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(quartetsNN, quartets_batch)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    trainEpoch.append(epoch)\n",
    "    trainLoss.append(train_loss)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    # print avg training statistics\n",
    "    train_accuracy_test = train_correct / train_total\n",
    "    print(\"correct predictions = %d \\ttotal predictions = %d \\tratio = %.3f\"%(\\\n",
    "         train_correct, train_total, train_accuracy_test))\n",
    "\n",
    "    train_loss = train_loss/len(dataloaderTrain)\n",
    "\n",
    "    print('Epoch: {} \\tLearning rate: {:.6f} \\tTraining Loss: {:.6f} \\tTime Elapsed: {:.6f}[s]'.format(\n",
    "        epoch,\n",
    "        optimizer.param_groups[0]['lr'],\n",
    "        train_loss,\n",
    "        end - start\n",
    "        ), flush=True)\n",
    "    if train_accuracy_test >max_train_accuracy:\n",
    "      max_train_accuracy = train_accuracy_test\n",
    "\n",
    "    # advance the step in the scheduler\n",
    "    exp_lr_scheduler.step()\n",
    "\n",
    "    if epoch % 10 == 0 :\n",
    "        test_epoch.append(epoch)\n",
    "        get_epoch.append(epoch)\n",
    "        # first case:\n",
    "        datasetTest = data.TensorDataset(inputTest[0:1000], outputTest[0:1000])\n",
    "        dataloaderTest = torch.utils.data.DataLoader(datasetTest,\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     shuffle=True)\n",
    "        model.eval()\n",
    "        test_loss_1, test_acc_1, max_accuracy_1 = test_acc(dataloaderTest, epoch, max_accuracy_1)\n",
    "        get_test1_loss.append(test_loss_1)\n",
    "        get_test1_acc.append(test_acc_1)\n",
    "\n",
    "        # second case:\n",
    "        datasetTest = data.TensorDataset(inputTest[1000:2000], outputTest[1000:2000])\n",
    "        dataloaderTest = torch.utils.data.DataLoader(datasetTest,\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     shuffle=True)\n",
    "        model.eval()\n",
    "        test_loss_2, test_acc_2, max_accuracy_2 = test_acc(dataloaderTest, epoch, max_accuracy_2)\n",
    "        get_test2_loss.append(test_loss_2)\n",
    "        get_test2_acc.append(test_acc_2)\n",
    "\n",
    "        # third case:\n",
    "        datasetTest = data.TensorDataset(inputTest[2000:3000], outputTest[2000:3000])\n",
    "        dataloaderTest = torch.utils.data.DataLoader(datasetTest,\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     shuffle=True)\n",
    "        model.eval()\n",
    "        test_loss_3, test_acc_3, max_accuracy_3 = test_acc(dataloaderTest, epoch, max_accuracy_3)\n",
    "        get_test3_loss.append(test_loss_3)\n",
    "        get_test3_acc.append(test_acc_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c0fc94-31fe-47ea-a614-ea40e5e128ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we add the statistics\n",
    "if not path.exists(summary_file):\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(\"{} \\t {} \\t {} \\t {} \\t {} \\t {} \\t {} \\t {}\\n\".format(\"Script name\",\n",
    "                                    \" Json file\",\n",
    "                                    \"label file\",\n",
    "                                    \"lerning rate\", \n",
    "                                    \"batch size\", \n",
    "                                    \"max testing accuracy\", \n",
    "                                    \"train loss\", \n",
    "                                    \"N epoch\", \n",
    "                                    \"chnl_dim\",\n",
    "                                    \"embd_dim\"))\n",
    "\n",
    "info_output = np.c_[get_epoch,get_test1_acc, get_test2_acc, get_test3_acc]\n",
    "np.savetxt(\"EpochAccuracy200.csv\", info_output, delimiter=\",\") \n",
    "test_output = np.c_[test_epoch, get_test1_loss, get_test2_loss, get_test3_loss]\n",
    "np.savetxt(\"test_loss_array200.csv\", test_output, delimiter=\",\") \n",
    "train_output = np.c_[trainEpoch,trainLoss]\n",
    "np.savetxt(\"trainLoss200.csv\", train_output, delimiter=\",\")\n",
    "\n",
    "with open(summary_file, 'a') as f:\n",
    "    f.write(\"{} \\t {} \\t {} \\t {} \\t {} \\t {} \\t {} \\t {} \\n\".format(nameScript.split(\".\")[0],\n",
    "                                    nameJson.split(\".\")[0],\n",
    "                                    label_files,\n",
    "                                    str(lr), \n",
    "                                    str(batch_size), \n",
    "                                    str(max_accuracy_1), \n",
    "                                    str(max_accuracy_2), \n",
    "                                    str(max_accuracy_3), \n",
    "                                    str(n_epochs)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
