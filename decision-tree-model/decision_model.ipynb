{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9df2049b-fda3-45fc-881d-7a9e564bac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672711e7-f3b9-4209-bea8-0d58a0a28f52",
   "metadata": {},
   "source": [
    "# Graph Class Definition\n",
    "\n",
    "It should consist of:\n",
    "\n",
    "- `nodes`: All current node in the existing tree with their embedding (V set)\n",
    "- `node_types`: all nodes in the existing tree with either leaf type or internal type\n",
    "- `edge_source`: the start node of all existing edges\n",
    "- `edge_dest`: the end node of all existing edges\n",
    "- `leaf`: the remaining species the model could choose from for leaf nodes\n",
    "\n",
    "**No branch length implemented for now!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec1ba010-4291-457b-b2a8-e273d982f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self, batch_size, embed_size, num_taxon, device):\n",
    "        if batch_size is None:\n",
    "            return\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        # init all components in the graph\n",
    "        # existing nodes in the graph: each row contains the embedding vector of the original sequence for 1 node\n",
    "        self.nodes = torch.zeros(0, embed_size, dtype=torch.float32, device=device)\n",
    "        # type of the existing nodes: 1 dim array, each index is a type of the node, either internal nodes or leaf nodes\n",
    "        self.node_types = torch.zeros(0, dtype=torch.uint8, device=device)\n",
    "        # starting point of each node:1 dim array, number indicate the origin node of edge i\n",
    "        self.edge_source = torch.zeros(0, dtype=torch.long, device=device)\n",
    "        # ending point of each node:1 dim array, number indicate the destination node of edge i\n",
    "        self.edge_dest = torch.zeros(0, dtype=torch.long, device=device)\n",
    "        # all remaining species to choose from\n",
    "        self.leaf = torch.zeros(num_taxon, embed_size, device=device, dtype=torch.uint8)\n",
    "        \n",
    "        # FIXME\n",
    "        # No edge feature for now...\n",
    "        # not sure what it is for now, could be the graph selected for the current batch, will update\n",
    "        self.owner_masks = torch.zeros(batch_size, 0, dtype=torch.uint8, device=device)\n",
    "        self.last_inserted_node = torch.zeros(batch_size, dtype=torch.long, device=device)\n",
    "        # current running graph in the batch\n",
    "        self.running = torch.ones(batch_size, device=device, dtype=torch.uint8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61867fc-e00f-4845-b57e-925c74a6a557",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45039a8-f6d3-4358-9f9c-1bdd78015410",
   "metadata": {},
   "source": [
    "## Propagator:\n",
    "The message passing part of the model, we update the node vectors based on the current existing graph\n",
    "\n",
    "If no node or no edges existed in the tree, the following model will not run and directly return the current graph feature matrix\n",
    "\n",
    "For the layers of this part of the model:\n",
    "\n",
    "1. `message_node`: get the node message from the vector embedding (embed_size -> message_size)\n",
    "   - The message that comes out of this layer will be a vector of message_size for each existing nodes\n",
    "   - The message that feeds into the next layer will be the addition of source and destination nodes from these vectors\n",
    "3. `message_layer`: addition layer for `message_node` (message_size -> message_size)\n",
    "4. `node_update_fn`: The Gated Recurrent Unit cells that pass the message along neighboring nodes (message_size -> embed_size)\n",
    "\n",
    "The return matrix is the updated feature matrix of the existing tree.\n",
    "\n",
    "**Unsure if how exactly `_reset_parameters` works, will investigate once the model is runnable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ab8b490-d84d-43e2-a3f6-981da7cea22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Propagator(torch.nn.Module):\n",
    "    def __init__(self, embed_size, dropout):\n",
    "        super().__init__()\n",
    "        # The message size in the message-passing\n",
    "        self.message_size = embed_size * 2\n",
    "        # update all node vectors back to original embed size\n",
    "        self.node_update_fn = torch.nn.GRUCell(self.message_size, embed_size)\n",
    "\n",
    "        # Get the node message through a linear layer\n",
    "        self.message_node = torch.nn.Linear(embed_size, self.message_size, bias=False)\n",
    "        # second layer of message passing\n",
    "        self.message_layer = torch.nn.Sequential(\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(self.message_size, self.message_size)\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self._reset_parameters(embed_size)\n",
    "\n",
    "    @staticmethod\n",
    "    def _node_update_mask(graph: Graph, mask_override: torch.ByteTensor):\n",
    "        return graph.owner_masks[graph.running if mask_override is None else mask_override].sum(0)>0\n",
    "\n",
    "    def forward(self, graph: Graph, mask_override: torch.ByteTensor = None):\n",
    "        # no node or edge in the graph, no need for message passing\n",
    "        if graph.nodes.shape[0]==0 or graph.edge_source.shape[0]==0:\n",
    "            return graph\n",
    "        # get all node features from embedding\n",
    "        node_features = self.message_node(graph.node)\n",
    "        # get the source and destestion node features\n",
    "        e1 = node_features.index_select(dim=0, index=graph.edge_source)\n",
    "        e2 = node_features.index_select(dim=0, index=graph.edge_dest)\n",
    "        messages = e1 + e2\n",
    "        messages = self.message_layer(messages)\n",
    "        messages = self.dropout(dropout)\n",
    "\n",
    "        # concatnate the messages for all nodes\n",
    "        # now the matrix contains a vector for each existing nodes, a result of addition of its destination and source nodes\n",
    "        inputs = torch.zeros(graph.nodes.shape[0], self.message_size, device=graph.nodes.device,\n",
    "                             dtype=graph.nodes.dtype).index_add_(0, graph.edge_dest, messages).\\\n",
    "                             index_add_(0, graph.edge_source, messages)\n",
    "\n",
    "        inputs = self.dropout(inputs)\n",
    "        \n",
    "        # now we do message passing\n",
    "        updated_nodes = self.node_update_fn(inputs, graph.nodes)\n",
    "        # put the updated node into the graph set\n",
    "        # only update the masked node, keep the unmasked one as original\n",
    "        graph.nodes = torch.where(self._node_update_mask(graph, mask_override).unsqueeze(-1), updated_nodes, graph.nodes)\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def _reset_parameters(self, embed_size):\n",
    "        msg_gain = torch.nn.init.calculate_gain(\"tanh\")\n",
    "        #FIXME: not sure if it should be embed_size*3 or embed_size*2\n",
    "        xavier_init(self.message_node, msg_gain, embed_size * 3, self.message_size)\n",
    "        xavier_init(self.message_layer[1], 1)\n",
    "\n",
    "        self.node_update_fn.bias_hh.data.fill_(0)\n",
    "        self.node_update_fn.bias_ih.data.fill_(0)\n",
    "        self.node_update_fn.bias_hh[:embed_size].data.fill_(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dd20e6-4d7d-4c1f-9b59-3d944d0a2121",
   "metadata": {},
   "source": [
    "## Multilayer Propagator:\n",
    "This class simply run propagator multiple times, the number of iteration is decided by me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2be61ea5-fc5b-4b32-a7d3-06e4a31502e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPropagator(torch.nn.Module):\n",
    "    def __init__(self, embed_size, n_iter, dropout):\n",
    "        super().__init__()\n",
    "        ## run propagators for n_iter times\n",
    "        self.propagators = torch.nn.ModuleList([Propagator(embed_size, dropout) for i in range(n_iter)])\n",
    "\n",
    "    def forward(self, graph: Graph, *args, **kwargs):\n",
    "        for p in self.propagators:\n",
    "            graph = p(graph, *args, **kwargs)\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73baba2a-38ee-4f2a-9e35-c4d868e6f269",
   "metadata": {},
   "source": [
    "## Aggregator:\n",
    "This part of the model get the propagated node vectors and aggregate them into 1 single vector of higher dimension\n",
    "\n",
    "The output of this model will be used as inputs for all the decision making NN in the later part of the bigger model\n",
    "\n",
    "For the layers of this model:\n",
    "\n",
    "1. `aggregate`: map the embedded(propagated) node vectors to a higher dimension (embed_size -> aggregated_size)\n",
    "2. `gated_sum`: gating vector for the gated sum (embed_size -> aggregated_size)\n",
    "\n",
    "What comes out of the above 2 NN is 2 `num_node * aggregated_size` matrices $g_{v}$ and $h_{v}$. We do dot product first, and add all rows of the dot product together, ie $\\sum_{v\\in V} g_{v} \\odot h_{v}$ \n",
    "\n",
    "In the end, it becomes one `1*aggregated_size` vector for each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20706a17-eaf2-434e-a59f-028dba20411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregator(torch.nn.Module):\n",
    "    def __init__(self, embed_size, dropout, bias_if_empty=False):\n",
    "        super.__init__()\n",
    "\n",
    "        self.aggregated_size = embed_size * 2\n",
    "        # map embedding to a higher dimension\n",
    "        self.aggregate = torch.nn.Linear(embed_size, aggregated_size)\n",
    "        # part of the gated sum\n",
    "        self.gated_sum = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embed_size, aggregated_size),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "  \n",
    "        self.bias_if_empty = torch.nn.Parameter(torch.Tensor(1, aggregated_size)) if bias_if_empty else None\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def forward(self, graph: Graph):\n",
    "        # if no nodes exists in the current tree, return a vector full of zeros or the bias\n",
    "        if graph.nodes.shape[0] == 0:\n",
    "            if self.bias_if_empty is not None:\n",
    "                return self.bias_if_empty.expand(graph.batch_size, -1)\n",
    "            else:\n",
    "                return torch.zeros(graph.batch_size, self.aggregated_size, dtype=torch.float32, device=graph.device)\n",
    "\n",
    "        \n",
    "        gates = self.gated_sum(graph.nodes)\n",
    "        feature = self.aggregate(graph.nodes)\n",
    "\n",
    "        # get the gated sum from the 2 NN\n",
    "        fmask = graph.owner_masks.float()\n",
    "        gated_sum = torch.mm(fmask, feature * gates)\n",
    "\n",
    "        return self.dropout(gated_sum)\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        xavier_init(self.transform, 1)\n",
    "        xavier_init(self.gate[0], 1)\n",
    "        self.gate[0].bias.data.fill_(1)\n",
    "        if self.bias_if_empty is not None:\n",
    "            torch.nn.init.normal_(self.bias_if_empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36861bf-b8db-44e6-b6da-6085d8e922a2",
   "metadata": {},
   "source": [
    "## Add_Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1378f1-a078-4744-ad1f-c63fe85221a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add_Node(torch.nn.Module):\n",
    "    def __init__(self, embed_size, aggregated_size, propagate_steps, dropout):\n",
    "        super().__init__()\n",
    "        self.propagator = MultilayerPropagator(embed_size, propagate_steps, dropout)\n",
    "        self.decision_aggregator = Aggregator(embed_size, aggregated_size, dropout, bias_if_empty=True)\n",
    "        self.generate_aggregator = Aggregator(embed_size, aggregated_size, dropout, bias_if_empty=True)\n",
    "\n",
    "        # Decide whether to:\n",
    "        # - Not add any node: 0\n",
    "        # - Add an internal node: 1\n",
    "        # - Add a leaf node: 2\n",
    "        self.node_decision = torch.nn.Linear(aggregated_size, 3)\n",
    "        # get the parameter of the node embedding\n",
    "        self.node_embedding = torch.nn.Parameter(torch.Tensor(embed_size))\n",
    "        # NN for generating the internal nodes\n",
    "        self.init_1 = torch.nn.Linear(embed_size, embed_size)\n",
    "        self.init_2 = torch.nn.Linear(aggregated_size, embed_size, bias=False)\n",
    "\n",
    "        self._reset_parameters(embed_size, aggregated_size)\n",
    "\n",
    "    def forward(self, graph:Graph):\n",
    "        loss = 0\n",
    "        # do a message passing before start the decision\n",
    "        graph = self.propagator(graph)\n",
    "        # make decision: add node or not? if so, what type?\n",
    "        new_node_type = self.node_decision(self.decision_aggregator(graph))\n",
    "        # Force model to add node if existing tree is empty\n",
    "        if graph.node.shape[0] == 0:\n",
    "            new_node_type[:, 0] = float(\"-inf\")\n",
    "\n",
    "        selected_node_type = sample_softmax(new_node_type)\n",
    "        # if selected type is 0, terminate the whole algorithm\n",
    "        graph.running = (selected_node_type != 0) & graph.running\n",
    "        if graph.running.any():\n",
    "            # Leaf node\n",
    "            if selected_node_type == 1:\n",
    "                #do leaf node thingy\n",
    "            else:\n",
    "                # internal nodes\n",
    "                new_embedding = self.node_embedding\n",
    "                # get the vector representing the whole graph \n",
    "                init_feature = self.generate_aggregator(graph)\n",
    "                # We generate the feature for this internal node based on the existing\n",
    "                new_feature = self.init_1(new_embedding) + self.init_2(init_feature)\n",
    "\n",
    "                # Now we have the node feature for the new node, we add it\n",
    "                mask = graph.running\n",
    "                index_seq = torch.arange(mask.long().sum(), device = graph.device, dtype = torch.long) + \\\n",
    "                        (graph.nodes.shape[0] if graph.nodes is not None else 0)\n",
    "                last_nodes = torch.zeros(graph.batch_size, device = graph.device, dtype = torch.long)\n",
    "                last_nodes[mask] = index_seq\n",
    "\n",
    "                # Select last node if updated\n",
    "                graph.last_inserted_node = torch.where(mask, last_nodes, graph.last_inserted_node)\n",
    "                # Select the new generated node features \n",
    "                new_node = new_feature[mask]\n",
    "                # So here is how this line of code works...\n",
    "                # mask is all the graph in the batch that is currently running, eg:[0,0,1,1,1,0,0,0,0,0]\n",
    "                # mask.nonzero() gives the index of all the nonzero values, eg:[[2], [3], [4]]\n",
    "                # mask.nonzero().squeeze(-1) remove the last dimension, eg: [2,3,4]\n",
    "                # With the one_hot and batch_size classes, gives a one_hot matrix, eg: [0,0,1,0,0,0,0,0,0,0]\n",
    "                #                                                                      [0,0,0,1,0,0,0,0,0,0]\n",
    "                #                                                                      [0,0,0,0,1,0,0,0,0,0]\n",
    "                # transpose() is just the transpose of this matrix\n",
    "                owner_mask = F.one_hot(mask.nonzero().squeeze(-1), graph.batch_size).transpose(0,1).byte()\n",
    "                graph.nodes = torch.cat((graph.nodes, new_nodes), dim=0)\n",
    "                graph.owner_masks = torch.cat((graph.owner_masks, owner_masks), dim=1)\n",
    "                \n",
    "        return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7d887cf-edf8-449e-adc2-95f3b814a7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(layer, scale, n_inputs=None, n_outputs=None):\n",
    "    n_inputs = n_inputs if n_inputs is not None else layer.weight.shape[1]\n",
    "    n_outputs = n_outputs if n_outputs is not None else layer.weight.shape[0]\n",
    "    limits = scale * math.sqrt(6.0 / (n_inputs + n_outputs))\n",
    "    layer.weight.data.uniform_(-limits, limits)\n",
    "\n",
    "    if layer.bias is not None:\n",
    "        torch.nn.init.normal_(layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "051fbcc0-6016-42c9-ac23-26d3fc364520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_softmax(tensor, dim=-1):\n",
    "    eps=1e-20\n",
    "\n",
    "    # Built in gumbel softmax could end up with lots of nans. Do it manually here.\n",
    "    noise = -torch.log(-torch.log(torch.rand_like(tensor)+eps) + eps)\n",
    "    res = F.softmax(tensor + noise, dim=-1)\n",
    "    _, res = res.max(dim=dim)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4008cd77-5f97-47b9-94f6-75593fbdc1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [1, 0, 1, 0],\n",
       "        [0, 1, 0, 1],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(10, dtype=torch.uint8)\n",
    "a[2] = 1\n",
    "a[3] = 1\n",
    "\n",
    "x = F.one_hot(a.nonzero().squeeze(-1), 10).transpose(0,1).byte()\n",
    "y = F.one_hot(a.nonzero().squeeze(-1), 10).transpose(0,1).byte()\n",
    "torch.cat((x, y), dim=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
