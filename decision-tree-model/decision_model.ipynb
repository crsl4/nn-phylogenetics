{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9df2049b-fda3-45fc-881d-7a9e564bac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672711e7-f3b9-4209-bea8-0d58a0a28f52",
   "metadata": {},
   "source": [
    "# Graph Class Definition\n",
    "\n",
    "It should consist of:\n",
    "\n",
    "- `nodes`: All current node in the existing tree with their embedding (V set)\n",
    "- `node_types`: all nodes in the existing tree with either leaf type or internal type\n",
    "- `edge_source`: the start node of all existing edges\n",
    "- `edge_dest`: the end node of all existing edges\n",
    "- `leaf`: the remaining species the model could choose from for leaf nodes\n",
    "\n",
    "**No branch length implemented for now!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ec1ba010-4291-457b-b2a8-e273d982f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self, batch_size, embed_size, num_taxon, device):\n",
    "        if batch_size is None:\n",
    "            return\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        # init all components in the graph\n",
    "        # existing nodes in the graph: each row contains the embedding vector of the original sequence for 1 node\n",
    "        self.nodes = torch.zeros(0, embed_size, dtype=torch.float32, device=device)\n",
    "        # type of the existing nodes: 1 dim array, each index is a type of the node, either internal nodes or leaf nodes\n",
    "        self.node_types = torch.zeros(0, dtype=torch.uint8, device=device)\n",
    "        # starting point of each node:1 dim array, number indicate the origin node of edge i\n",
    "        self.edge_source = torch.zeros(0, dtype=torch.long, device=device)\n",
    "        # ending point of each node:1 dim array, number indicate the destination node of edge i\n",
    "        self.edge_dest = torch.zeros(0, dtype=torch.long, device=device)\n",
    "        # all remaining species to choose from\n",
    "        self.leaf = torch.zeros(num_taxon, embed_size, dtype=torch.float32, device=device)\n",
    "        \n",
    "        # FIXMEleaf\n",
    "        # No edge feature for now...\n",
    "        # not sure what it is for now, could be the graph selected for the current batch, will update\n",
    "        self.owner_masks = torch.zeros(batch_size, 0, dtype=torch.uint8, device=device)\n",
    "        self.last_inserted_node = torch.zeros(batch_size, dtype=torch.long, device=device)\n",
    "        # current running graph in the batch\n",
    "        self.running = torch.ones(batch_size, device=device, dtype=torch.uint8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61867fc-e00f-4845-b57e-925c74a6a557",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45039a8-f6d3-4358-9f9c-1bdd78015410",
   "metadata": {},
   "source": [
    "## Propagator:\n",
    "The message passing part of the model, we update the node vectors based on the current existing graph\n",
    "\n",
    "If no node or no edges existed in the tree, the following model will not run and directly return the current graph feature matrix\n",
    "\n",
    "For the layers of this part of the model:\n",
    "\n",
    "1. `message_node`: get the node message from the vector embedding (embed_size -> message_size)\n",
    "   - The message that comes out of this layer will be a vector of message_size for each existing nodes\n",
    "   - The message that feeds into the next layer will be the addition of source and destination nodes from these vectors\n",
    "3. `message_layer`: addition layer for `message_node` (message_size -> message_size)\n",
    "4. `node_update_fn`: The Gated Recurrent Unit cells that pass the message along neighboring nodes (message_size -> embed_size)\n",
    "\n",
    "We also include reverse message passing here.\n",
    "\n",
    "The return matrix is the updated feature matrix of the existing tree.\n",
    "\n",
    "**Unsure if how exactly `_reset_parameters` works, will investigate once the model is runnable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2ab8b490-d84d-43e2-a3f6-981da7cea22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Propagator(torch.nn.Module):\n",
    "    def __init__(self, embed_size, dropout):\n",
    "        super().__init__()\n",
    "        # The message size in the message-passing\n",
    "        self.message_size = embed_size * 2\n",
    "        # update all node vectors back to original embed size\n",
    "        self.node_update_fn = torch.nn.GRUCell(self.message_size, embed_size)\n",
    "\n",
    "        # Get the node message through a linear layer\n",
    "        self.message_node = torch.nn.Linear(embed_size, self.message_size, bias=False)\n",
    "        # second layer of message passing\n",
    "        self.message_layer = torch.nn.Sequential(\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(self.message_size, self.message_size)\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self._reset_parameters(embed_size)\n",
    "\n",
    "    @staticmethod\n",
    "    def _node_update_mask(graph: Graph, mask_override: torch.ByteTensor):\n",
    "        return graph.owner_masks[graph.running if mask_override is None else mask_override].sum(0)>0\n",
    "\n",
    "    def forward(self, graph: Graph, mask_override: torch.ByteTensor = None):\n",
    "        # no node or edge in the graph, no need for message passing\n",
    "        if graph.nodes.shape[0]==0 or graph.edge_source.shape[0]==0:\n",
    "            return graph\n",
    "        # get all node features from embedding\n",
    "        node_features = self.message_node(graph.node)\n",
    "        # get the source and destestion node features\n",
    "        e1 = node_features.index_select(dim=0, index=graph.edge_source)\n",
    "        e2 = node_features.index_select(dim=0, index=graph.edge_dest)\n",
    "        messages = e1 + e2\n",
    "        messages = self.message_layer(messages)\n",
    "        messages = self.dropout(dropout)\n",
    "        \n",
    "        # concatnate the messages for all nodes\n",
    "        # now the matrix contains a vector for each existing nodes, a result of addition of its destination and source nodes\n",
    "        inputs = torch.zeros(graph.nodes.shape[0], self.message_size, device=graph.nodes.device,\n",
    "                             dtype=graph.nodes.dtype).index_add_(0, graph.edge_dest, messages).\\\n",
    "                             index_add_(0, graph.edge_source, messages)\n",
    "\n",
    "        inputs = self.dropout(inputs)\n",
    "\n",
    "        # We also want reverse message passing\n",
    "        r1 = node_features.index_select(dim=0, index=graph.edge_dest)\n",
    "        r2 = node_features.index_select(dim=0, index=graph.edge_source)\n",
    "        reverse_messages = e1 + e2\n",
    "        reverse_messages = self.message_layer(reverse_messages)\n",
    "        reverse_messages = self.dropout(dropout)\n",
    "\n",
    "        reverse_inputs = torch.zeros(graph.nodes.shape[0], self.message_size, device=graph.nodes.device,\n",
    "                             dtype=graph.nodes.dtype).index_add_(0, graph.edge_source, reverse_messages).\\\n",
    "                             index_add_(0, graph.edge_dest, reverse_messages)\n",
    "\n",
    "        reverse_inputs = self.dropout(reverse_inputs)\n",
    "\n",
    "        # Add up both messages\n",
    "        full_input = inputs + reverse_inputs\n",
    "        \n",
    "        # now we do message passing\n",
    "        updated_nodes = self.node_update_fn(full_input, graph.nodes)\n",
    "        # put the updated node into the graph set\n",
    "        # only update the masked node, keep the unmasked one as original\n",
    "        graph.nodes = torch.where(self._node_update_mask(graph, mask_override).unsqueeze(-1), updated_nodes, graph.nodes)\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def _reset_parameters(self, embed_size):\n",
    "        msg_gain = torch.nn.init.calculate_gain(\"tanh\")\n",
    "        #FIXME: not sure if it should be embed_size*3 or embed_size*2\n",
    "        xavier_init(self.message_node, msg_gain, embed_size * 3, self.message_size)\n",
    "        xavier_init(self.message_layer[1], 1)\n",
    "\n",
    "        self.node_update_fn.bias_hh.data.fill_(0)\n",
    "        self.node_update_fn.bias_ih.data.fill_(0)\n",
    "        self.node_update_fn.bias_hh[:embed_size].data.fill_(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dd20e6-4d7d-4c1f-9b59-3d944d0a2121",
   "metadata": {},
   "source": [
    "## Multilayer Propagator:\n",
    "This class simply run propagator multiple times, the number of iteration is decided by me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2be61ea5-fc5b-4b32-a7d3-06e4a31502e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPropagator(torch.nn.Module):\n",
    "    def __init__(self, embed_size, n_iter, dropout):\n",
    "        super().__init__()\n",
    "        ## run propagators for n_iter times\n",
    "        self.propagators = torch.nn.ModuleList([Propagator(embed_size, dropout) for i in range(n_iter)])\n",
    "\n",
    "    def forward(self, graph: Graph, *args, **kwargs):\n",
    "        for p in self.propagators:\n",
    "            graph = p(graph, *args, **kwargs)\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73baba2a-38ee-4f2a-9e35-c4d868e6f269",
   "metadata": {},
   "source": [
    "## Aggregator:\n",
    "This part of the model get the propagated node vectors and aggregate them into 1 single vector of higher dimension\n",
    "\n",
    "The output of this model will be used as inputs for all the decision making NN in the later part of the bigger model\n",
    "\n",
    "For the layers of this model:\n",
    "\n",
    "1. `aggregate`: map the embedded(propagated) node vectors to a higher dimension (embed_size -> aggregated_size)\n",
    "2. `gated_sum`: gating vector for the gated sum (embed_size -> aggregated_size)\n",
    "\n",
    "What comes out of the above 2 NN is 2 `num_node * aggregated_size` matrices $g_{v}$ and $h_{v}$. We do dot product first, and add all rows of the dot product together, ie $\\sum_{v\\in V} g_{v} \\odot h_{v}$ \n",
    "\n",
    "In the end, it becomes one `1*aggregated_size` vector for each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "20706a17-eaf2-434e-a59f-028dba20411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregator(torch.nn.Module):\n",
    "    def __init__(self, embed_size, dropout, bias_if_empty=False):\n",
    "        super.__init__()\n",
    "\n",
    "        self.aggregated_size = embed_size * 2\n",
    "        # map embedding to a higher dimension\n",
    "        self.aggregate = torch.nn.Linear(embed_size, aggregated_size)\n",
    "        # part of the gated sum\n",
    "        self.gated_sum = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embed_size, aggregated_size),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "  \n",
    "        self.bias_if_empty = torch.nn.Parameter(torch.Tensor(1, aggregated_size)) if bias_if_empty else None\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def forward(self, graph: Graph):\n",
    "        # if no nodes exists in the current tree, return a vector full of zeros or the bias\n",
    "        if graph.nodes.shape[0] == 0:\n",
    "            if self.bias_if_empty is not None:\n",
    "                return self.bias_if_empty.expand(graph.batch_size, -1)\n",
    "            else:\n",
    "                return torch.zeros(graph.batch_size, self.aggregated_size, dtype=torch.float32, device=graph.device)\n",
    "\n",
    "        \n",
    "        gates = self.gated_sum(graph.nodes)\n",
    "        feature = self.aggregate(graph.nodes)\n",
    "\n",
    "        # get the gated sum from the 2 NN\n",
    "        fmask = graph.owner_masks.float()\n",
    "        gated_sum = torch.mm(fmask, feature * gates)\n",
    "\n",
    "        return self.dropout(gated_sum)\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        xavier_init(self.transform, 1)\n",
    "        xavier_init(self.gate[0], 1)\n",
    "        self.gate[0].bias.data.fill_(1)\n",
    "        if self.bias_if_empty is not None:\n",
    "            torch.nn.init.normal_(self.bias_if_empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36861bf-b8db-44e6-b6da-6085d8e922a2",
   "metadata": {},
   "source": [
    "## Add_Node\n",
    "\n",
    "This part of the model adds a node to the graph, it works as follows:\n",
    "\n",
    "1. Do a `propagator` before everything, retrieve the upgraded node vectors\n",
    "2. `decision_aggregator` give a single vector for the entire updated graph, and then pass it into `node_decision`. The output is either \"0\", for no node should be added; \"1\", a leaf node should be added; or \"2\", a internal node should be added\n",
    "3. If leaf node, get a logit value from the existing nodes and the species node, the NN should return a logit number for each species that we could choose from, then we do a softmax to choose a species, and remove the choosen one from the species set.\n",
    "4. If internal node, we generate the new node vector by combining the parameter of the new node NN and the aggregated vector for the existing graph.\n",
    "5. We then add the new node to the graph and update the owner mask.\n",
    "\n",
    "For the module of this part:\n",
    "\n",
    "1. `node_decision`: given a aggregated vector of the current graph, output a number from 0,1,2\n",
    "2. `node_embedding`: the output seems to be very very small number, maybe noise? or perhaps unnecessary.\n",
    "3. `init_1`: take `node_embedding` as input, output another vector with `embed_size`\n",
    "4. `init_2`: take the aggregated graph vector as input, combine with output of `init_1` gives the embedding for the newly generated internal node\n",
    "5. `leaf_decision_species`: Part of the decision of \"which species to choose from\" that comes from the species node embedding\n",
    "6. `leaf_decision_tree`: Part of the decision of \"which species to choose from\" that comes from the existing tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ad1378f1-a078-4744-ad1f-c63fe85221a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add_Node(torch.nn.Module):\n",
    "    def __init__(self, embed_size, aggregated_size, propagate_steps, dropout):\n",
    "        super().__init__()\n",
    "        self.propagator = MultilayerPropagator(embed_size, propagate_steps, dropout)\n",
    "        self.decision_aggregator = Aggregator(embed_size, aggregated_size, dropout, bias_if_empty=True)\n",
    "        self.generate_aggregator = Aggregator(embed_size, aggregated_size, dropout, bias_if_empty=True)\n",
    "        self.leaf_node_aggregator = Aggregator(embed_size, aggregated_size, dropout, bias_if_empty=True)\n",
    "        \n",
    "        # Decide whether to:\n",
    "        # - Not add any node: 0\n",
    "        # - Add an internal node: 1\n",
    "        # - Add a leaf node: 2\n",
    "        self.node_decision = torch.nn.Linear(aggregated_size, 3)\n",
    "        # get the parameter of the node embedding\n",
    "        self.node_embedding = torch.nn.Parameter(torch.Tensor(embed_size))\n",
    "        # NN for generating the internal nodes\n",
    "        self.init_1 = torch.nn.Linear(embed_size, embed_size)\n",
    "        self.init_2 = torch.nn.Linear(aggregated_size, embed_size, bias=False)\n",
    "\n",
    "        # Decide which species to choose\n",
    "        self.leaf_decision_species = torch.nn.Linear(embed_size, 1)\n",
    "        self.leaf_decision_tree = torch.nn.Linear(aggregated_size, 1, bias=False)\n",
    "        \n",
    "        self._reset_parameters(embed_size, aggregated_size)\n",
    "\n",
    "    def forward(self, graph:Graph):\n",
    "        loss = 0\n",
    "        running = graph.running\n",
    "        # do a message passing before start the decision\n",
    "        graph = self.propagator(graph)\n",
    "        # make decision: add node or not? if so, what type?\n",
    "        new_node_type = self.node_decision(self.decision_aggregator(graph))\n",
    "        # Force model to add node if existing tree is empty\n",
    "        if graph.node.shape[0] == 0:\n",
    "            new_node_type[:, 0] = float(\"-inf\")\n",
    "\n",
    "        selected_node_type = sample_softmax(new_node_type)\n",
    "        # if selected type is 0, terminate the whole algorithm\n",
    "        graph.running = (selected_node_type != 0) & graph.running\n",
    "        if graph.running.any():\n",
    "            # Leaf node\n",
    "            if selected_node_type == 1:\n",
    "                # aggregate the existing tree into 1 vector\n",
    "                species_aggregator = self.leaf_node_aggregator(graph)\n",
    "                # get a logit for each species in the set\n",
    "                # the logit will be a matrix of batch_size*num_species\n",
    "                logits = self.leaf_decision_species(graph.leaf).unsqueeze(0) + self.leaf_decision_tree(species_aggregator).unsqueeze(1)\n",
    "                logits = logits.view(logits.shape[0], -1)\n",
    "                # Dont really do anything, will remove if unnecessary\n",
    "                owner_mask_tmp = graph.owner_masks.unsqueeze(-1).expand(-1, -1, 1).contiguous().view(graph.batch_size, -1)\n",
    "                # get the index of the selected species\n",
    "                selected_species = masked_softmax(logits, owner_mask_tmp)\n",
    "                # get the actual embedding of the selected species\n",
    "                new_feature = graph.leaf[selected_species]\n",
    "                \n",
    "                # Now we need to remove the selected species from the species list\n",
    "                graph.leaf = torch.cat((graph.leaf[:selected_species], graph.leaf[selected_species+1:]))\n",
    "            else:\n",
    "                # internal nodes\n",
    "                new_embedding = self.node_embedding\n",
    "                # get the vector representing the whole graph \n",
    "                init_feature = self.generate_aggregator(graph)\n",
    "                # We generate the feature for this internal node based on the existing\n",
    "                new_feature = self.init_1(new_embedding) + self.init_2(init_feature)\n",
    "\n",
    "            # Now we have the node feature for the new node, we add it\n",
    "            mask = graph.running\n",
    "            index_seq = torch.arange(mask.long().sum(), device = graph.device, dtype = torch.long) + \\\n",
    "                    (graph.nodes.shape[0] if graph.nodes is not None else 0)\n",
    "            last_nodes = torch.zeros(graph.batch_size, device = graph.device, dtype = torch.long)\n",
    "            last_nodes[mask] = index_seq\n",
    "\n",
    "            # Select last node if updated\n",
    "            graph.last_inserted_node = torch.where(mask, last_nodes, graph.last_inserted_node)\n",
    "            # Select the new generated node features \n",
    "            new_node = new_feature[mask]\n",
    "            # So here is how this line of code works...\n",
    "            # mask is all the graph in the batch that is currently running, eg:[0,0,1,1,1,0,0,0,0,0]\n",
    "            # mask.nonzero() gives the index of all the nonzero values, eg:[[2], [3], [4]]\n",
    "            # mask.nonzero().squeeze(-1) remove the last dimension, eg: [2,3,4]\n",
    "            # With the one_hot and batch_size classes, gives a one_hot matrix, eg: [0,0,1,0,0,0,0,0,0,0]\n",
    "            #                                                                      [0,0,0,1,0,0,0,0,0,0]\n",
    "            #                                                                      [0,0,0,0,1,0,0,0,0,0]\n",
    "            # transpose() is just the transpose of this matrix\n",
    "            owner_mask = F.one_hot(mask.nonzero().squeeze(-1), graph.batch_size).transpose(0,1).byte()\n",
    "            graph.nodes = torch.cat((graph.nodes, new_nodes), dim=0)\n",
    "            graph.owner_masks = torch.cat((graph.owner_masks, owner_masks), dim=1)\n",
    "                \n",
    "        return graph\n",
    "\n",
    "    def _reset_parameters(self, embed_size, aggregated_size):\n",
    "        torch.nn.init.normal_(self.node_type_embedding)\n",
    "        xavier_init(self.init_1, 1, embed_size + aggregated_size, embed_size)\n",
    "        xavier_init(self.init_2, 1, embed_size + aggregated_size, embed_size)\n",
    "        xavier_init(self.node_decision, 1)\n",
    "        xavier_init(self.leaf_decision_species, 1, embed_size*2, 1)\n",
    "        xavier_init(self.leaf_decision_tree, 1, embed_size + aggregated_size, 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6b17d9-3605-44ba-b14d-265a736fb61c",
   "metadata": {},
   "source": [
    "## Add_Edge\n",
    "\n",
    "This part decide whether to add an edge, and where the edge should connect the last added node to the existing graph\n",
    "\n",
    "The mudules of this part work as follows:\n",
    "\n",
    "1. `addedge_existing`: Part of the \"add edge or not\" decision that comes from the already existing graph, takes the aggregated vector as input\n",
    "2. `addedge_new`: Part of the \"add edge or not\" decision that comes from the last added node (The new node), takes the node vector as input\n",
    "3. `where_existing`: Part of the \"where to add edge\" decision that comes from the existing graph, takes all node vectors as input\n",
    "4. `where_new`: Part of the \"where to add edge\" decision that comes from the new node.\n",
    "\n",
    "To know which node we are connecting the new node to, we get a vector of logits from `where` modules. In this vector, each number represents a logit value for each node in the graph, we then do softmax and select the biggest number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "efedc9cc-6d82-449f-8c57-7a2519982c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add_Edge(torch.nn.Module):\n",
    "    def __init__(self, embed_size, aggregated_size, propagate_steps, dropout):\n",
    "        super().__init()\n",
    "        self.n_max_edges = 2\n",
    "        self.propagator = MultilayerPropagator(embed_size, propagate_steps, dropout)\n",
    "        self.edge_decision_aggregator = Aggregator(embed_size, aggregated_size, dropout)\n",
    "        \n",
    "        self.addedge_existing = torch.nn.Linear(aggregated_size, 1)\n",
    "        self.addedge_new = torch.nn.Linear(embed_size, 1, bias=False)\n",
    "\n",
    "        self.where_existing = torch.nn.Linear(embed_size, 1)\n",
    "        self.where_new = torch.nn.Linear(embed_size, 1, bias=False)\n",
    "\n",
    "        self._reset_parameters(embed_size, aggregated_size)\n",
    "\n",
    "    def forward(self, graph: Graph):\n",
    "        running = graph.running\n",
    "        #Current added edge, should not exceed 2\n",
    "        added_edge = 0\n",
    "        new_node = graph.nodes.index_select(0, graph.last_inserted_node)\n",
    "\n",
    "        # Decision process to add edges until termination\n",
    "        while True:\n",
    "            # A round of messing passing\n",
    "            graph = self.propagator(graph, running)\n",
    "            # need to consider both existing graph and the current new node\n",
    "            new_edge_to_add = (self.addedge_existing(self.edge_decision_aggregator(graph)) +\n",
    "                               self.addedge_new(new_nodes)).squeeze(-1)\n",
    "\n",
    "            # give a true/false on if a new edge is needed or not\n",
    "            add_or_not = sample_binary(new_edge_to_add)\n",
    "\n",
    "            # Check if there are already 2 edges added\n",
    "            if added_edge >= self.n_max_edges:\n",
    "                add_or_not = torch.zeros_like(add_or_not)\n",
    "\n",
    "            # termination condition\n",
    "            running = running & add_or_not\n",
    "            if not running.any():\n",
    "                break\n",
    "\n",
    "            # edge_logit -> [batch_size, num_nodes]\n",
    "            # We do softmax on it and get the biggest number\n",
    "            edge_logit = self.where_existing(graph.nodes).unsqueeze(0) + self.where_new(new_node).unsqueeze(1)\n",
    "            edge_logit = edge_logit.view(edge_logit.shape[0], -1)\n",
    "\n",
    "            # I don't think this line does anyting -- it returns the original owner_mask, but leave it for now\n",
    "            owner_mask = graph.owner_mask.unsqueeze(-1).contiguous().view(graph.batch_size, -1)\n",
    "            # Do the softmax, selected node is the index of the node that will be connected to\n",
    "            selected_node = masked_softmax(edge_logit, owner_mask)\n",
    "\n",
    "            # now we add the edge to the set\n",
    "            selected_dest = graph.last_inserted_node[running]\n",
    "            selected_src = selected_node[running]\n",
    "\n",
    "            graph.edge_dest = torch.cat((graph.edge_dest, selected_dest), 0)\n",
    "            graph.edge_source = torch.cat((graph.edge_source, selected_src), 0)\n",
    "            add_index += 1\n",
    "        \n",
    "        return graph\n",
    "\n",
    "    def _reset_paramters(self, embed_size, aggregated_size):\n",
    "        xavier_init(self.addedge_existing, 1, embed_size + aggregated_size, 1)\n",
    "        xavier_init(self.addedge_new, 1, embed_size + aggregated_size, 1)\n",
    "        xavier_init(self.where_existing, 1, embed_size * 2, 1)\n",
    "        xavier_init(self.where_new, 1, embed_size * 2, 1)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f838211e-06f5-4076-80f3-f904a60e33c9",
   "metadata": {},
   "source": [
    "## Tree_Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4f461e-340f-40c5-a86e-73dd3fb5ec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree_Generation(torch.nn.Module):\n",
    "    def __init__(self, embed_size, propagate_steps, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.aggregated_size = embed_size*2\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.add_edge = Add_Edge(embed_size, self.aggregated_size, propagate_steps, dropout)\n",
    "        self.add_node = Add_Node(embed_size, self.aggregated_size, propagate_steps, dropout)\n",
    "\n",
    "    def forward(self, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a10039-8214-4488-a6b8-315a2be587af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f7d887cf-edf8-449e-adc2-95f3b814a7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(layer, scale, n_inputs=None, n_outputs=None):\n",
    "    n_inputs = n_inputs if n_inputs is not None else layer.weight.shape[1]\n",
    "    n_outputs = n_outputs if n_outputs is not None else layer.weight.shape[0]\n",
    "    limits = scale * math.sqrt(6.0 / (n_inputs + n_outputs))\n",
    "    layer.weight.data.uniform_(-limits, limits)\n",
    "\n",
    "    if layer.bias is not None:\n",
    "        torch.nn.init.normal_(layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "051fbcc0-6016-42c9-ac23-26d3fc364520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_softmax(tensor, dim=-1):\n",
    "    eps=1e-20\n",
    "\n",
    "    # Built in gumbel softmax could end up with lots of nans. Do it manually here.\n",
    "    noise = -torch.log(-torch.log(torch.rand_like(tensor)+eps) + eps)\n",
    "    res = F.softmax(tensor + noise, dim=-1)\n",
    "    _, res = res.max(dim=dim)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "55ab2686-42f6-4f3f-8369-bd596718393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_binary(tensor):\n",
    "    tensor = torch.sigmoid(tensor)\n",
    "    return torch.rand_like(tensor) < tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "28943073-d1c8-4bc1-b714-4ba89a6c6744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_softmax_input(tensor, mask):\n",
    "    return torch.where(mask, tensor, torch.full([1], float(\"-inf\"), dtype=tensor.dtype, device=tensor.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6bde4451-c44a-4eaf-93fe-19bf69e03c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(tensor, mask):\n",
    "    tensor = mask_softmax_input(tensor, mask)\n",
    "    return sample_softmax(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0bd2d1-a3d5-495d-a559-8265fceb39ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4d57ebab-ca4a-49da-859f-e3d887452fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([[0.1, 0.3, 0.2], [0.5, 0.3, 0.11], [0.02, 0.8, 0.13]])\n",
    "target = torch.tensor([1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4008cd77-5f97-47b9-94f6-75593fbdc1f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8649)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remap_pad(t, pad_char, transform = lambda x: x+1):\n",
    "    return torch.where(t != pad_char, transform(t), torch.zeros(1, dtype=t.dtype, device=t.device))\n",
    "F.cross_entropy(t, target.long())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
