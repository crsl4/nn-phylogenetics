{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9df2049b-fda3-45fc-881d-7a9e564bac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672711e7-f3b9-4209-bea8-0d58a0a28f52",
   "metadata": {},
   "source": [
    "# Graph Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec1ba010-4291-457b-b2a8-e273d982f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self, batch_size, embed_size, device):\n",
    "        if batch_size is None:\n",
    "            return\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        # init all components in the graph\n",
    "        # existing nodes in the graph: each row contains the embedding vector of the original sequence for 1 node\n",
    "        self.nodes = torch.zeros(0, embed_size, dtype=torch.float32, device=device)\n",
    "        # type of the existing nodes: 1 dim array, each index is a type of the node, either internal nodes or leaf nodes\n",
    "        self.node_types = torch.zeros(0, dtype=torch.uint8, device=device)\n",
    "        # starting point of each node:1 dim array, number indicate the origin node of edge i\n",
    "        self.edge_source = torch.zeros(0, dtype=torch.long, device=device)\n",
    "        # ending point of each node:1 dim array, number indicate the destination node of edge i\n",
    "        self.edge_dest = torch.zeros(0, dtype=torch.long, device=device)\n",
    "        \n",
    "        # FIXME\n",
    "        # No edge feature for now...\n",
    "        # not sure what it is for now, could be the graph selected for the current batch, will update\n",
    "        self.owner_masks = torch.zeros(batch_size, 0, dtype=torch.uint8, device=device)\n",
    "        self.last_inserted_node = torch.zeros(batch_size, dtype=torch.long, device=device)\n",
    "        # current running graph in the batch\n",
    "        self.running = torch.ones(batch_size, device=device, dtype=torch.uint8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61867fc-e00f-4845-b57e-925c74a6a557",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45039a8-f6d3-4358-9f9c-1bdd78015410",
   "metadata": {},
   "source": [
    "## Propagator:\n",
    "The message passing part of the model, we update the node vectors based on the current existing graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ab8b490-d84d-43e2-a3f6-981da7cea22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Propagator(torch.nn.Module):\n",
    "    def __init__(self, embed_size, dropout):\n",
    "        super().__init__()\n",
    "        # The message size in the message-passing\n",
    "        self.message_size = embed_size * 2\n",
    "        # update all node vectors back to original embed size\n",
    "        self.node_update_fn = torch.nn.GRUCell(self.message_size, embed_size)\n",
    "\n",
    "        # Get the node message through a linear layer\n",
    "        self.message_node = torch.nn.Linear(embed_size, self.message_size, bias=False)\n",
    "        # second layer of message passing\n",
    "        self.message_layer = torch.nn.Sequential(\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(self.message_size, self.message_size)\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self._reset_parameters(embed_size)\n",
    "\n",
    "    @staticmethod\n",
    "    def _node_update_mask(graph: Graph, mask_override: torch.ByteTensor):\n",
    "        return graph.owner_masks[graph.running if mask_override is None else mask_override].sum(0)>0\n",
    "\n",
    "    def forward(self, graph: Graph, mask_override: torch.ByteTensor = None):\n",
    "        # no node or edge in the graph, no need for message passing\n",
    "        if graph.nodes.shape[0]==0 or graph.edge_source==0:\n",
    "            return graph\n",
    "        # get all node features from embedding\n",
    "        node_features = self.message_node(graph.node)\n",
    "        # get the source and destestion node features\n",
    "        e1 = node_features.index_select(dim=0, index=graph.edge_source)\n",
    "        e2 = node_features.index_select(dim=0, index=graph.edge_dest)\n",
    "        messages = e1 + e2\n",
    "        messages = self.message_layer(messages)\n",
    "        messages = self.dropout(dropout)\n",
    "\n",
    "        # concatnate the messages for all nodes\n",
    "        inputs = torch.zeros(graph.nodes.shape[0], self.message_size, device=graph.nodes.device,\n",
    "                             dtype=graph.nodes.dtype).index_add_(0, graph.edge_dest, messages).\\\n",
    "                             index_add_(0, graph.edge_source, messages)\n",
    "\n",
    "        inputs = self.dropout(inputs)\n",
    "        \n",
    "        # now we do message passing\n",
    "        updated_nodes = self.node_update_fn(inputs, graph.nodes)\n",
    "        # put the updated node into the graph set\n",
    "        # only update the masked node, keep the unmasked one as original\n",
    "        graph.nodes = torch.where(self._node_update_mask(graph, mask_override).unsqueeze(-1), updated_nodes, graph.nodes)\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def reset_param()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d887cf-edf8-449e-adc2-95f3b814a7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
