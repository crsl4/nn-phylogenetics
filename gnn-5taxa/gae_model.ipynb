{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea66606",
   "metadata": {},
   "source": [
    "# Import Packages and environmental setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6f1490b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import ete3\n",
    "import torch\n",
    "import mlflow.pytorch\n",
    "torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "import itertools\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from os import path\n",
    "sys.path.insert(0, '../')\n",
    "import gc\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn.conv import TransformerConv\n",
    "from torch_geometric.nn import VGAE\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import batched_negative_sampling\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_geometric.nn import BatchNorm\n",
    "from torch.nn import BatchNorm1d\n",
    "from torch.nn import Linear\n",
    "from ete3 import Tree\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b7eac9",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a075fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcfa2b8",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d632127c",
   "metadata": {},
   "source": [
    "The internal node will be in the order of 5-7-6, the single tip will always be connected to node 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26c885d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert string to numbers\n",
    "def convert_string_to_numbers(str, dict):\n",
    "    ''' str: string to convert\n",
    "        dict dictionary with the relative ordering of each char'''\n",
    "            # create a map iterator using a lambda function\n",
    "    # lambda x -> return dict[x]\n",
    "    # This return the value for each key in dict based on str\n",
    "    numbers = map(lambda x: dict[x], str)\n",
    "    # return an array of int64 numbers\n",
    "    return np.fromiter(numbers, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25656dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create all node features in a graph\n",
    "def get_all_nodes(idx):\n",
    "    node_str = []  # all amino acid sequence in a graph\n",
    "    site_count = [[] for _ in range(seq_length)]\n",
    "    for i in range(5):\n",
    "        # Count number of amino acid at each site\n",
    "        # get the index of the sequence from the original dataset\n",
    "        seq_idx = 5*idx + i\n",
    "        node_str.append(seq_string[seq_idx][:-1])\n",
    "        for j in range(seq_length):\n",
    "            site_count[j].append(seq_string[seq_idx][j])\n",
    "\n",
    "    node_in = []\n",
    "\n",
    "    for i in range(seq_length):\n",
    "        node_in.append(random.choice(site_count[i]))\n",
    "\n",
    "    internal_node = ''.join(node_in)\n",
    "\n",
    "    for i in range(3):\n",
    "        node_str.append(internal_node)\n",
    "    \n",
    "    return node_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53011b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a graph for each \n",
    "def construct_single_graph(idx, t):\n",
    "    ''' idx: the current graph index w.r.t the label\n",
    "        t: the tree object from ete'''\n",
    "    # transform the character of amino acid in to numbers for all 5 sequences in this graph\n",
    "    all_node = []\n",
    "    all_node = get_all_nodes(idx)\n",
    "    transformed_x = []\n",
    "    for node_idx in range(len(all_node)):\n",
    "        transformed_x.append(convert_string_to_numbers(all_node[node_idx], dict_amino))\n",
    "    \n",
    "    # Work out the branch distance from the Newick format\n",
    "    leaf_pair = 0               # The amount of leaf pair so far, max=2\n",
    "    prev_leaf = False           # Whether the previous leaf in the preorder is a leaf node\n",
    "    prev_dist = 0               # The distance of the branch coming out of the preivous node in preorder\n",
    "    dist_array = [0]*8          # The distance for outgoing branch for each node, node 7 will always be 0\n",
    "    prev_index = -1             # The index of the last leaf node in the preorder\n",
    "    tot_in_node = 0             # All distance of internal nodes that are unassigned so far\n",
    "    pending = False             # Some condition for assigning branch length that I don't remember\n",
    "    preorder=[]                 # The preorder of all leaf nodes\n",
    "    \n",
    "    # Traverse through all nodes in preorder, work out the branch distance \n",
    "    # There are only 2 possible rooted tree format from ETE, \n",
    "    # so 2 if statements that work out all different scenarios\n",
    "    for node in t.traverse(\"preorder\"):\n",
    "        if not node.name=='':\n",
    "            index = int(node.name) - 1\n",
    "            preorder.append(index)\n",
    "            dist_array[index] = node.dist\n",
    "            prev_index = index\n",
    "            if leaf_pair >= 2:\n",
    "                tot_in_node += node.dist\n",
    "                dist_array[index] = tot_in_node\n",
    "                break\n",
    "            else:\n",
    "                if prev_leaf==False:\n",
    "                    prev_leaf=True\n",
    "                else:\n",
    "                    leaf_pair += 1\n",
    "                    prev_leaf=False\n",
    "                    if prev_dist != 0:\n",
    "                        dist_array[leaf_pair+4] = prev_dist\n",
    "                    else:\n",
    "                        pending = True\n",
    "                    tot_in_node-=prev_dist\n",
    "        else:\n",
    "            prev_dist = node.dist\n",
    "            tot_in_node+=node.dist\n",
    "            if pending==True:\n",
    "                pending = False\n",
    "                prev_dist = 0\n",
    "                tot_in_node -= node.dist\n",
    "                dist_array[leaf_pair+4] = node.dist\n",
    "            if prev_leaf==True:\n",
    "                dist_array[prev_index] += node.dist\n",
    "                prev_dist = 0\n",
    "                tot_in_node -= node.dist\n",
    "    # Set up the adjency Matrix in COO format\n",
    "    # We find the smaller node number of each side.\n",
    "    # In this case, the tip with the larger node number is on the left side, thus connect to node 5\n",
    "    if min(preorder[0], preorder[1]) > min(preorder[2], preorder[3]):\n",
    "        # change edge value of edge 5 and 6\n",
    "        # I think this is due to the conditions from the previous part, but I don't remember the details\n",
    "        # It works though!\n",
    "        tmp = dist_array[5]\n",
    "        dist_array[5] = dist_array[6]\n",
    "        dist_array[6] = tmp\n",
    "        # Assign edge origin/destination and value\n",
    "        edge_index = torch.tensor([[preorder[2],5],[5,preorder[2]],[preorder[3],5],[5,preorder[3]],\n",
    "                                       [5,7],[7,5],[preorder[4],7],[7,preorder[4]],\n",
    "                                       [6,7],[7,6],[preorder[0],6],[6,preorder[0]],\n",
    "                                       [preorder[1],6],[6,preorder[1]]], dtype=torch.long)\n",
    "        \"\"\"\"\n",
    "        edge_attr = [dist_array[preorder[2]], dist_array[preorder[2]], \n",
    "                 dist_array[preorder[3]], dist_array[preorder[3]], \n",
    "                 dist_array[5], dist_array[5],\n",
    "                 dist_array[preorder[4]], dist_array[preorder[4]],\n",
    "                 dist_array[6], dist_array[6],\n",
    "                 dist_array[preorder[0]], dist_array[preorder[0]],\n",
    "                 dist_array[preorder[1]], dist_array[preorder[1]]]\n",
    "        \"\"\"\n",
    "    # Same thing, but now the smaller node number is on the left, thus connected with node 5\n",
    "    else:\n",
    "        edge_index = torch.tensor([[preorder[0],5],[5,preorder[0]],[preorder[1],5],[5,preorder[1]],\n",
    "                                       [5,7],[7,5],[preorder[4],7],[7,preorder[4]],\n",
    "                                       [6,7],[7,6],[preorder[2],6],[6,preorder[2]],\n",
    "                                       [preorder[3],6],[6,preorder[3]]], dtype=torch.long)\n",
    "        \"\"\"\n",
    "        edge_attr = [dist_array[preorder[0]], dist_array[preorder[0]], \n",
    "                 dist_array[preorder[1]], dist_array[preorder[1]], \n",
    "                 dist_array[5], dist_array[5],\n",
    "                 dist_array[preorder[4]], dist_array[preorder[4]],\n",
    "                 dist_array[6], dist_array[6],\n",
    "                 dist_array[preorder[2]], dist_array[preorder[2]],\n",
    "                 dist_array[preorder[3]], dist_array[preorder[3]]]\n",
    "        \"\"\"\n",
    "       \n",
    "    concat_x = np.array( transformed_x )\n",
    "    # create the node feature vector\n",
    "    x = torch.tensor(concat_x, dtype=torch.float)\n",
    "    # Now we create the graph object as Data\n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35f2d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f51b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the correct adjacency matrix of one graph\n",
    "def select_graph_original(graph_id, batch_targets, batch_index):\n",
    "    # create a true/false mask to select the graph we want from the all dense adj matrix of the whole batch\n",
    "    graph_mask = torch.eq(batch_index, graph_id)\n",
    "    graph_targets = batch_targets[graph_mask][:, graph_mask]\n",
    "    triu_indices = torch.triu_indices(graph_targets.shape[0], graph_targets.shape[0], offset=1)\n",
    "    triu_mask = torch.squeeze(to_dense_adj(triu_indices)).bool()\n",
    "    return graph_targets[triu_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6c2ed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the predicted adjacency matrix of one graph\n",
    "def select_graph_prediction(triu_logit, graph_size, start):\n",
    "    graph_triu_logit = torch.squeeze(triu_logit[start:start + graph_size])\n",
    "    return graph_triu_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d4737a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss(mu=None, logstd=None):\n",
    "    MAX_LOGSTD = 10\n",
    "    logstd =  logstd.clamp(max=MAX_LOGSTD)\n",
    "    kl_div = -0.5 * torch.mean(torch.sum(1 + 2 * logstd - mu**2 - logstd.exp()**2, dim=1))\n",
    "\n",
    "    # Limit numeric errors\n",
    "    kl_div = kl_div.clamp(max=1000)\n",
    "    return kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dca5e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(triu_logits, edge_index, mu, logvar, batch_index, kl_beta):\n",
    "    # Convert target edge index to dense adjacency matrix\n",
    "    # This is the actual adj matrix for the whole batch, converted directly from edge index\n",
    "    batch_targets = torch.squeeze(to_dense_adj(edge_index))\n",
    "    batch_recon_loss = []         # the loss for each graph in this batch\n",
    "    batch_node_counter = 0        # track which node are we in the current batch\n",
    "    for graph_id in torch.unique(batch_index):\n",
    "        # get the actual upper triangular adjacency matrix for this graph\n",
    "        graph_true_triu = select_graph_original(graph_id, batch_targets, batch_index)\n",
    "        # get the prediction of adjac matrix for this graph\n",
    "        graph_predict_triu =  select_graph_prediction(triu_logits, graph_true_triu.shape[0], batch_node_counter)\n",
    "        # update node counter\n",
    "        batch_node_counter = batch_node_counter + graph_true_triu.shape[0]\n",
    "        # Calculate edge-weighted binary cross entropy\n",
    "        weight = graph_true_triu.shape[0]/sum(graph_true_triu)\n",
    "        bce = torch.nn.BCEWithLogitsLoss(pos_weight=weight).to(device)\n",
    "        graph_recon_loss = bce(graph_predict_triu.view(-1), graph_true_triu.view(-1))\n",
    "        batch_recon_loss.append(graph_recon_loss)\n",
    "        \n",
    "    # Take average of all losses\n",
    "    num_graphs = torch.unique(batch_index).shape[0]\n",
    "    batch_recon_loss = sum(batch_recon_loss) / num_graphs\n",
    "    # KL Divergence\n",
    "    kl_divergence = kl_loss(mu, logvar)\n",
    "    return batch_recon_loss + kl_beta * kl_divergence, kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa8da38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_triu_graph_reconstruction(graph_predictions_triu, graph_targets_triu, num_nodes=None):\n",
    "    # Apply sigmoid to get binary prediction values\n",
    "    preds = (torch.sigmoid(graph_predictions_triu.view(-1)) > 0.5).int()\n",
    "    # Reshape the ground truth\n",
    "    labels = graph_targets_triu.view(-1)\n",
    "     # Check if the predictions and the groundtruth match\n",
    "    if labels.shape[0] == sum(torch.eq(preds, labels)):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b91dc561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the accuracy for each epoch (both train and test)\n",
    "def reconstruction_accuracy(triu_logits, edge_index, batch_index):\n",
    "    batch_targets = torch.squeeze(to_dense_adj(edge_index))\n",
    "    batch_targets_triu = []\n",
    "    # Iterate over batch and collect each of the trius\n",
    "    batch_node_counter = 0\n",
    "    num_recon = 0\n",
    "    for graph_id in torch.unique(batch_index):\n",
    "        # Get triu parts for this graph\n",
    "        graph_targets_triu = select_graph_original(graph_id, \n",
    "                                                batch_targets, \n",
    "                                                batch_index)\n",
    "        graph_predictions_triu = select_graph_prediction(triu_logits, \n",
    "                                                        graph_targets_triu.shape[0], \n",
    "                                                        batch_node_counter)\n",
    "        # Update counter to the index of the next graph\n",
    "        batch_node_counter = batch_node_counter + graph_targets_triu.shape[0]\n",
    "        # Check if graph is successfully reconstructed\n",
    "        num_nodes = sum(torch.eq(batch_index, graph_id))\n",
    "        recon = check_triu_graph_reconstruction(graph_predictions_triu, \n",
    "                                                graph_targets_triu, \n",
    "                                                num_nodes) \n",
    "        num_recon = num_recon + int(recon)\n",
    "        batch_targets_triu.append(graph_targets_triu)\n",
    "    \n",
    "    batch_targets_triu = torch.cat(batch_targets_triu)\n",
    "    triu_discrete = torch.squeeze(torch.tensor(torch.sigmoid(triu_logits) > 0.5, dtype=torch.int32))\n",
    "    acc = torch.true_divide(torch.sum(batch_targets_triu==triu_discrete), batch_targets_triu.shape[0]) \n",
    "        \n",
    "    return acc.detach().cpu().numpy(), num_recon    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01cd70db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_testing_set(idx):\n",
    "    one_set = []\n",
    "    all_node = []\n",
    "    all_node = get_all_nodes(idx)\n",
    "    transformed_x = []\n",
    "    for node_idx in range(len(all_node)):\n",
    "        transformed_x.append(convert_string_to_numbers(all_node[node_idx], dict_amino))\n",
    "    concat_x = np.array( transformed_x )\n",
    "    x = torch.tensor(concat_x, dtype=torch.float)\n",
    "    # 0\n",
    "    edge_index = torch.tensor([[0,5],[5,0],[1,5],[5,1],\n",
    "                                   [5,7],[7,5],[4,7],[7,4],\n",
    "                                   [7,6],[6,7],[2,6],[6,2],\n",
    "                                   [3,6],[6,3]], dtype=torch.long)\n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "    one_set.append(data)\n",
    "    # 1\n",
    "    edge_index = torch.tensor([[0,5],[5,0],[1,5],[5,1],\n",
    "                                   [5,7],[7,5],[3,7],[7,3],\n",
    "                                   [7,6],[6,7],[2,6],[6,2],\n",
    "                                   [4,6],[6,4]], dtype=torch.long)\n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "    one_set.append(data)\n",
    "    # 2\n",
    "    edge_index = torch.tensor([[0,5],[5,0],[1,5],[5,1],\n",
    "                                   [5,7],[7,5],[2,7],[7,2],\n",
    "                                   [7,6],[6,7],[3,6],[6,3],\n",
    "                                   [4,6],[6,4]], dtype=torch.long)\n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "    one_set.append(data)\n",
    "    # 3\n",
    "    edge_index = torch.tensor([[0,5],[5,0],[2,5],[5,2],\n",
    "                                   [5,7],[7,5],[4,7],[7,4],\n",
    "                                   [7,6],[6,7],[1,6],[6,1],\n",
    "                                   [3,6],[6,3]], dtype=torch.long)\n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "    one_set.append(data)\n",
    "    # 4\n",
    "    edge_index = torch.tensor([[0,5],[5,0],[2,5],[5,2],\n",
    "                                   [5,7],[7,5],[3,7],[7,3],\n",
    "                                   [7,6],[6,7],[1,6],[6,1],\n",
    "                                   [4,6],[6,4]], dtype=torch.long)\n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "    one_set.append(data)\n",
    "    # 5\n",
    "    edge_index = torch.tensor([[0,5],[5,0],[2,5],[5,2],\n",
    "                                   [5,7],[7,5],[1,7],[7,1],\n",
    "                                   [7,6],[6,7],[3,6],[6,3],\n",
    "                                   [4,6],[6,4]], dtype=torch.long)\n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "    one_set.append(data)\n",
    "    # 6\n",
    "    edge_index = torch.tensor([[0,5],[5,0],[3,5],[5,3],\n",
    "                                   [5,7],[7,5],[4,7],[7,4],\n",
    "                                   [7,6],[6,7],[1,6],[6,1],\n",
    "                                   [2,6],[6,2]], dtype=torch.long)\n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "    one_set.append(data)\n",
    "    # 7\n",
    "    edge_index = torch.tensor([[0,5],[5,0],[3,5],[5,3],\n",
    "                                   [5,7],[7,5],[2,7],[7,2],\n",
    "                                   [7,6],[6,7],[1,6],[6,1],\n",
    "                                   [4,6],[6,4]], dtype=torch.long)\n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "    one_set.append(data)\n",
    "    # 8\n",
    "    edge_index = torch.tensor([[0,5],[5,0],[3,5],[5,3],\n",
    "                                   [5,7],[7,5],[1,7],[7,1],\n",
    "                                   [7,6],[6,7],[2,6],[6,2],\n",
    "                                   [4,6],[6,4]], dtype=torch.long)\n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "    one_set.append(data)\n",
    "    # 9\n",
    "    edge_index = torch.tensor([[0,5],[5,0],[4,5],[5,4],\n",
    "                                   [5,7],[7,5],[3,7],[7,3],\n",
    "                                   [7,6],[6,7],[1,6],[6,1],\n",
    "                                   [2,6],[6,2]], dtype=torch.long)\n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "    one_set.append(data)\n",
    "    # 10\n",
    "    edge_index = torch.tensor([[0,5],[5,0],[4,5],[5,4],\n",
    "                                   [5,7],[7,5],[2,7],[7,2],\n",
    "                                   [7,6],[6,7],[1,6],[6,1],\n",
    "                                   [3,6],[6,3]], dtype=torch.long)\n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "    one_set.append(data)\n",
    "    # 11\n",
    "    edge_index = torch.tensor([[0,5],[5,0],[4,5],[5,4],\n",
    "                                   [5,7],[7,5],[1,7],[7,1],\n",
    "                                   [7,6],[6,7],[2,6],[6,2],\n",
    "                                   [3,6],[6,3]], dtype=torch.long)\n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "    one_set.append(data)\n",
    "    # 12\n",
    "    edge_index = torch.tensor([[1,5],[5,1],[2,5],[5,2],\n",
    "                                   [5,7],[7,5],[0,7],[7,0],\n",
    "                                   [7,6],[6,7],[3,6],[6,3],\n",
    "                                   [4,6],[6,4]], dtype=torch.long)\n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "    one_set.append(data)\n",
    "    # 13\n",
    "    edge_index = torch.tensor([[1,5],[5,1],[3,5],[5,3],\n",
    "                                   [5,7],[7,5],[0,7],[7,0],\n",
    "                                   [7,6],[6,7],[2,6],[6,2],\n",
    "                                   [4,6],[6,4]], dtype=torch.long)\n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "    one_set.append(data)\n",
    "    # 14\n",
    "    edge_index = torch.tensor([[1,5],[5,1],[4,5],[5,4],\n",
    "                                   [5,7],[7,5],[0,7],[7,0],\n",
    "                                   [7,6],[6,7],[2,6],[6,2],\n",
    "                                   [3,6],[6,3]], dtype=torch.long)\n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "    one_set.append(data)\n",
    "\n",
    "    return one_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0d73de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_val_epoch(data_loader, epoch, kl_beta, true_label):\n",
    "    loss_group = []\n",
    "    acc_group = []\n",
    "    idx = 0\n",
    "    correct_prediction = 0\n",
    "    # 15 graph is a group\n",
    "    counter = 0\n",
    "    for i, batch in enumerate(tqdm(data_loader)):\n",
    "        try:    \n",
    "            counter += 1\n",
    "            batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            triu_logits, mu, logvar = model(batch.x.float(), batch.edge_index, batch.batch)\n",
    "            loss, kl_loss = loss_func(triu_logits, batch.edge_index, mu, logvar, batch.batch, kl_beta)\n",
    "            acc, num_recon = reconstruction_accuracy(triu_logits, batch.edge_index, batch.batch)\n",
    "            loss_group.append(loss.detach().cpu().numpy())\n",
    "            acc_group.append(acc)\n",
    "            \n",
    "            \n",
    "            if counter == 15:\n",
    "                counter = 0\n",
    "                biggest_acc = max(acc_group)\n",
    "                all_max_acc = [i for i in range(len(acc_group)) if acc_group[i] == biggest_acc]\n",
    "                min_loss = 100\n",
    "                predicted = -1\n",
    "                \n",
    "                if idx < 20:\n",
    "                    print(f\"Acc: {acc_group}\")\n",
    "                    print(f\"Loss: {loss_group}\")\n",
    "                \n",
    "                for k in all_max_acc:\n",
    "                    if loss_group[k] < min_loss:\n",
    "                        min_loss = loss_group[k]\n",
    "                        predicted = k + 1\n",
    "                \n",
    "                acc_group = []\n",
    "                loss_group = []\n",
    "                if true_label[idx] == predicted:\n",
    "                    print(f\"Correct label: {predicted}\")\n",
    "                    correct_prediction += 1\n",
    "                idx += 1\n",
    "                \n",
    "        except IndexError as error:\n",
    "            print(\"Error: \", error)\n",
    "    \n",
    "    print(f\"epoch {epoch} space search accuracy: {correct_prediction/1000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "150d3627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_epoch(data_loader, type, epoch, kl_beta):\n",
    "    all_losses = []\n",
    "    all_accs = []\n",
    "    all_kldivs = []\n",
    "    \n",
    "    reconstructed_tree = 0\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(data_loader)):\n",
    "        try:\n",
    "            batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            triu_logits, mu, logvar = model(batch.x.float(), batch.edge_index, batch.batch)\n",
    "            loss, kl_loss = loss_func(triu_logits, batch.edge_index, mu, logvar, batch.batch, kl_beta)\n",
    "            if type == \"Train\":\n",
    "                loss.backward()  \n",
    "                optimizer.step()  \n",
    "            # Calculate metrics\n",
    "            acc, num_recon = reconstruction_accuracy(triu_logits, batch.edge_index, batch.batch)\n",
    "            reconstructed_tree = reconstructed_tree + num_recon\n",
    "            \n",
    "            all_losses.append(loss.detach().cpu().numpy())\n",
    "            all_accs.append(acc)\n",
    "            all_kldivs.append(kl_loss.detach().cpu().numpy())\n",
    "        except IndexError as error:\n",
    "            print(\"Error: \", error)\n",
    "    \n",
    "    print(f\"{type} epoch {epoch} loss: \", np.array(all_losses).mean())\n",
    "    print(f\"{type} epoch {epoch} accuracy: \", np.array(all_accs).mean())\n",
    "    print(f\"Reconstructed {reconstructed_tree}.\")\n",
    "    mlflow.log_metric(key=f\"{type} Epoch Loss\", value=float(np.array(all_losses).mean()), step=epoch)\n",
    "    mlflow.log_metric(key=f\"{type} Epoch Accuracy\", value=float(np.array(all_accs).mean()), step=epoch)\n",
    "    mlflow.log_metric(key=f\"{type} Num Reconstructed\", value=float(reconstructed_tree), step=epoch)\n",
    "    mlflow.log_metric(key=f\"{type} KL Divergence\", value=float(np.array(all_kldivs).mean()), step=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ace10d2",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8e8a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GVAE(nn.Module):\n",
    "    def __init__(self, feature_size, embedding_size):\n",
    "        super(GVAE, self).__init__()\n",
    "        self.latent_embedding_size = int(embedding_size/2)\n",
    "        decoder_size = embedding_size*4\n",
    "        \n",
    "        # Encoder Layers\n",
    "        # 3 layers with batch normalization\n",
    "        self.conv1 = TransformerConv(feature_size,\n",
    "                                    embedding_size*4,\n",
    "                                    heads=4,\n",
    "                                    concat=False,\n",
    "                                    beta=True)\n",
    "\n",
    "        self.bn1 = BatchNorm(embedding_size*4)\n",
    "        self.conv2 = TransformerConv(embedding_size*4,\n",
    "                                    embedding_size*2,\n",
    "                                    heads=4,\n",
    "                                    concat=False,\n",
    "                                    beta=True)\n",
    "                                            \n",
    "        self.bn2 = BatchNorm(embedding_size*2)\n",
    "        self.conv3 = TransformerConv(embedding_size*2,\n",
    "                                    embedding_size,\n",
    "                                    heads=4,\n",
    "                                    concat=False,\n",
    "                                    beta=True)\n",
    "                                            \n",
    "        self.bn3 = BatchNorm(embedding_size)\n",
    "        \n",
    "        # Latent transform\n",
    "        self.mu_transform = TransformerConv(embedding_size, \n",
    "                                            self.latent_embedding_size,\n",
    "                                            heads=4,\n",
    "                                            concat=False,\n",
    "                                            beta=True)\n",
    "                                            \n",
    "        self.logvar_transform = TransformerConv(embedding_size, \n",
    "                                            self.latent_embedding_size,\n",
    "                                            heads=4,\n",
    "                                            concat=False,\n",
    "                                            beta=True)\n",
    "                                            \n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_dense_1 = Linear(self.latent_embedding_size*2, decoder_size)\n",
    "        self.decoder_bn_1 = BatchNorm1d(decoder_size)\n",
    "        self.decoder_dense_2 = Linear(self.latent_embedding_size*2, decoder_size)\n",
    "        self.decoder_bn_2 = BatchNorm1d(decoder_size)\n",
    "        self.decoder_dense_3 = Linear(self.latent_embedding_size*2, decoder_size)\n",
    "        self.decoder_bn_3 = BatchNorm1d(decoder_size)\n",
    "        self.decoder_dense_4 = Linear(decoder_size, 1)\n",
    "    \n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.bn2(x)\n",
    "        x = self.conv3(x, edge_index).relu()\n",
    "        x = self.bn3(x)\n",
    "        # latent variable\n",
    "        mu = self.mu_transform(x, edge_index)\n",
    "        logvar = self.logvar_transform(x, edge_index)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def decode(self, z, batch_index):\n",
    "        # z: the 5 latent vectors for all graph\n",
    "        # batch\n",
    "        inputs = []\n",
    "        \n",
    "        # for each graph in this batch\n",
    "        for graph_id in torch.unique(batch_index):\n",
    "            # Select the latent vectors for the graphs in this batch\n",
    "            graph_mask = torch.eq(batch_index, graph_id)\n",
    "            graph_z = z[graph_mask]\n",
    "            # Get upper triangle adjacency matrix\n",
    "            # the diagonal is not included (the node is always connected to itself)\n",
    "            # should be something like this:\n",
    "            # [0,0,0,0,0,0,0,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,4,4,4,5,5,6],\n",
    "            # [1,2,3,4,5,6,7,2,3,4,5,6,7,3,4,5,6,7,4,5,6,7,5,6,7,6,7,7]\n",
    "            # Note that each column is a pair of possible connection. Now we have all possible connection\n",
    "            edge_indices = torch.triu_indices(graph_z.shape[0], graph_z.shape[0], offset=1)\n",
    "            # We want to put actual latent vectors in the place.\n",
    "            # i.e, replace 0 in the previous array with latent vector for node 0.\n",
    "            dim = self.latent_embedding_size\n",
    "            # create the shape for source and target\n",
    "            # each should be a 28*dim (length of the edge_indices[0]) array with same values for each row indicating the node number\n",
    "            source_indices = torch.reshape(edge_indices[0].repeat_interleave(dim), (edge_indices.shape[1], dim))\n",
    "            target_indices = torch.reshape(edge_indices[1].repeat_interleave(dim), (edge_indices.shape[1], dim))\n",
    "            # Get the latent vectors\n",
    "            # should fill the previous arrays with actual latent vectors such as \n",
    "            # [-1, 0, 3.2, ..., -2.1, 0] (latent vector for node 0)\n",
    "            # [-1, 0, 3.2, ..., -2.1, 0]\n",
    "            #        ......              (28 rows)\n",
    "            # [0.3, 4.2, ..., 8, 2.5, 5] (latent vector for node 6)\n",
    "            sources_latent = torch.gather(graph_z, 0, source_indices.to(device))\n",
    "            target_latent = torch.gather(graph_z, 0, target_indices.to(device))\n",
    "            # Should be 28(pairs of node) * (2*dim)\n",
    "            graph_inputs = torch.cat([sources_latent, target_latent], axis=1)\n",
    "            inputs.append(graph_inputs)\n",
    "        \n",
    "        # now we concat all graphs in this batch\n",
    "        inputs = torch.cat(inputs)\n",
    "        # feed into the decoding layers\n",
    "        x = self.decoder_dense_1(inputs).relu()\n",
    "        x = self.decoder_bn_1(x)\n",
    "        x = self.decoder_dense_2(inputs).relu()\n",
    "        x = self.decoder_bn_2(x)\n",
    "        x = self.decoder_dense_3(inputs).relu()\n",
    "        x = self.decoder_bn_3(x)\n",
    "        edge_logits = self.decoder_dense_4(x)\n",
    "        \n",
    "        # We transform the logits later for probabilities\n",
    "        return edge_logits\n",
    "    \n",
    "    # transform mu and logvar into the latent vectors\n",
    "    def reparam(self, mu, logvar):\n",
    "        if self.training:\n",
    "            # transform logvar\n",
    "            std = torch.exp(logvar)\n",
    "            # generate same amount of random numbers from N(0, 1)\n",
    "            eps = torch.randn_like(std)\n",
    "            # get the sampled value\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "    def forward(self, x, edge_index, batch_index):\n",
    "        # GNN layers\n",
    "        mu, logvar = self.encode(x, edge_index)\n",
    "        # latent vectors\n",
    "        z = self.reparam(mu, logvar)\n",
    "        # Decode layers\n",
    "        logit = self.decode(z, batch_index)\n",
    "        \n",
    "        return logit, mu, logvar\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad071d",
   "metadata": {},
   "source": [
    "# File inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f2eb7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training the Garph Auto Encoder for 5-taxa dataset\n",
      "------------------------------------------------------------------------\n",
      "Executing gae_model.py following gae.json\n",
      "------------------------------------------------------------------------\n",
      "Loading Sequence Data in sequences12062021.in\n",
      "Loading Label Data in labels12062021.in\n",
      "Loading Tree Data in trees12062021.in\n",
      "Number of samples:10000; Sequence length of each sample:1550\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# get name of the script\n",
    "# nameScript = sys.argv[0].split('/')[-1]\n",
    "nameScript = \"gae_model.py\"\n",
    "# get json file name of the script\n",
    "nameJson = \"gae.json\"\n",
    "# nameJson = sys.argv[1]\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(\"Training the Garph Auto Encoder for 5-taxa dataset\")\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(\"Executing \" + nameScript + \" following \" + nameJson, flush = True)\n",
    "\n",
    "# opening Json file \n",
    "jsonFile = open(nameJson) \n",
    "dataJson = json.load(jsonFile)\n",
    "\n",
    "# loading the input data from the json file\n",
    "ngpu = dataJson[\"ngpu\"]                  # number of GPUS\n",
    "lr = dataJson[\"lr\"]                      # learning rate\n",
    "embedSize = dataJson[\"embedSize\"]        # Embedding size\n",
    "nEpochs = dataJson[\"nEpochs\"]            # Number of Epochs\n",
    "batchSize = dataJson[\"batchSize\"]        # batchSize\n",
    "kl_beta = dataJson[\"klBeta\"]\n",
    "\n",
    "data_root = dataJson[\"dataRoot\"]         # data folder\n",
    "model_root = dataJson[\"modelRoot\"]       # folder to save the data\n",
    "\n",
    "label_files = dataJson[\"labelFile\"]      # file with labels\n",
    "sequence_files = dataJson[\"matFile\"]     # file with sequences\n",
    "tree_files = dataJson[\"treeFile\"]        # file with tree structure\n",
    "\n",
    "if \"summaryFile\" in dataJson:\n",
    "    summary_file = dataJson[\"summaryFile\"]\n",
    "else :\n",
    "    summary_file = \"summary_file.txt\"\n",
    "\n",
    "\n",
    "print(\"------------------------------------------------------------------------\") \n",
    "print(\"Loading Sequence Data in \" + sequence_files, flush = True)\n",
    "print(\"Loading Label Data in \" + label_files, flush = True)\n",
    "print(\"Loading Tree Data in \" + tree_files, flush = True)\n",
    "\n",
    "# we read the labels as list of strings\n",
    "with open(data_root+label_files, 'r') as f:\n",
    "    label_char = f.readlines()\n",
    "\n",
    "# we read the sequence as a list of strings\n",
    "with open(data_root+sequence_files, 'r') as f:\n",
    "    seq_string = f.readlines()\n",
    "\n",
    "with open(data_root+tree_files, 'r') as f:\n",
    "    tree_newick = f.readlines()\n",
    "    \n",
    "labels = np.fromiter(map(lambda x: int(x[0])-1,\n",
    "                         label_char), dtype= np.int64)\n",
    "val_labels = labels[9000:10000]\n",
    "\n",
    "n_samples = len(label_char)\n",
    "seq_length = len(seq_string[0])-1\n",
    "print(\"Number of samples:{}; Sequence length of each sample:{}\"\n",
    "        .format(n_samples, seq_length))\n",
    "print(\"------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a06525",
   "metadata": {},
   "source": [
    "# Data pre-processing\n",
    "Read Sequence data and Newick tree format, return the all graph object with necessary info in the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e05e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to extract the dictionary with the relative positions\n",
    "# for each aminoacid\n",
    "\n",
    "# first we need to extract all the different chars\n",
    "strL = \"\"\n",
    "for c in seq_string[0][:-1]:\n",
    "    if not c in strL:\n",
    "        strL += c\n",
    "\n",
    "# we sort them\n",
    "strL = sorted(strL)\n",
    "\n",
    "# we give them a relative order\n",
    "dict_amino = {}\n",
    "for ii, c in enumerate(strL):\n",
    "    dict_amino[c] = ii\n",
    "\n",
    "# looping over the labels and create array. Here each element of the\n",
    "# label_char has the form \"1\\n\", so we only take the first one\n",
    "labels = np.fromiter(map(lambda x: int(x[0])-1,\n",
    "                         label_char), dtype= np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "766ca993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all graphs from raw dataset\n",
    "# EXTREMELY SLOW\n",
    "dataset = []  # empty dataset for all graphs\n",
    "# loop through all samples\n",
    "for i in range(n_samples):\n",
    "    # Get the ete tree format\n",
    "    tree = tree_newick[i][:-1]\n",
    "    t = Tree(tree)\n",
    "    # get node feature, COO adjacency matrix, and edge feature\n",
    "    data = construct_single_graph(i, t)\n",
    "    # Validate if number of node and edges match\n",
    "    if (not data.validate(raise_on_error=True)):\n",
    "        print(\"Error! Node number and edge set does not match!\")\n",
    "        break\n",
    "    # Add the graph into the dataset\n",
    "    dataset.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "451a0b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = []\n",
    "for i in range(9000, 10000):\n",
    "    one_dataset = construct_testing_set(i)\n",
    "    for data in one_dataset:\n",
    "        if (not data.validate(raise_on_error=True)):\n",
    "            print(\"Error! Node number and edge set does not match!\")\n",
    "            break\n",
    "        val_dataset.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a9d0c2",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2994f4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start the training process...\n",
      "------------------------------------------------------------------------\n",
      "Data loaded, loading model...\n",
      "Number of parameters in the model:  7454561\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/282 [00:00<?, ?it/s]/tmp/ipykernel_6246/3221043966.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  triu_discrete = torch.squeeze(torch.tensor(torch.sigmoid(triu_logits) > 0.5, dtype=torch.int32))\n",
      "100%|█████████████████████████████████████████| 282/282 [01:38<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 1 loss:  1.1843593\n",
      "Train epoch 1 accuracy:  0.34278825\n",
      "Reconstructed 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 282/282 [01:43<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 2 loss:  0.5599936\n",
      "Train epoch 2 accuracy:  0.80878377\n",
      "Reconstructed 987.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▎                                         | 9/282 [00:04<02:02,  2.23it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, nEpochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m): \n\u001b[1;32m     20\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mrun_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkl_beta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkl_beta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvluating testset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 16\u001b[0m, in \u001b[0;36mrun_one_epoch\u001b[0;34m(data_loader, type, epoch, kl_beta)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     15\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()  \n\u001b[0;32m---> 16\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[1;32m     18\u001b[0m acc, num_recon \u001b[38;5;241m=\u001b[39m reconstruction_accuracy(triu_logits, batch\u001b[38;5;241m.\u001b[39medge_index, batch\u001b[38;5;241m.\u001b[39mbatch)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py:410\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    408\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 410\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    412\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Start the training process...\")\n",
    "train_dataset = dataset[:9000]\n",
    "test_dataset = dataset[9000:]\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchSize, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batchSize, shuffle=True)\n",
    "validate_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(\"Data loaded, loading model...\")\n",
    "# Load model\n",
    "model = GVAE(seq_length, embedSize)\n",
    "print(\"Number of parameters in the model: \", count_parameters(model))\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=10, gamma=0.9)\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    for epoch in range(1, nEpochs+1): \n",
    "        model.train()\n",
    "        run_one_epoch(train_loader, type=\"Train\", epoch=epoch, kl_beta=kl_beta)\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Evluating testset...\")\n",
    "            model.eval()\n",
    "            run_one_epoch(test_loader, type=\"Test\", epoch=epoch, kl_beta=kl_beta)\n",
    "        if epoch % 25 == 0:\n",
    "            print(\"Running tree space search...\")\n",
    "            model.eval()\n",
    "            run_val_epoch(validate_loader, epoch=epoch, kl_beta=kl_beta, true_label=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70ffd0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
