{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea66606",
   "metadata": {},
   "source": [
    "# Import Packages and environmental setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6f1490b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import ete3\n",
    "import torch\n",
    "import mlflow.pytorch\n",
    "torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "import itertools\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from os import path\n",
    "sys.path.insert(0, '../')\n",
    "import gc\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn.conv import TransformerConv\n",
    "from torch_geometric.nn import VGAE\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import batched_negative_sampling\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_geometric.nn import BatchNorm\n",
    "from torch.nn import BatchNorm1d\n",
    "from torch.nn import Linear\n",
    "from ete3 import Tree\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b7eac9",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a075fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcfa2b8",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d632127c",
   "metadata": {},
   "source": [
    "The internal node will be in the order of 5-7-6, the single tip will always be connected to node 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26c885d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert string to numbers\n",
    "def convert_string_to_numbers(str, dict):\n",
    "    ''' str: string to convert\n",
    "        dict dictionary with the relative ordering of each char'''\n",
    "            # create a map iterator using a lambda function\n",
    "    # lambda x -> return dict[x]\n",
    "    # This return the value for each key in dict based on str\n",
    "    numbers = map(lambda x: dict[x], str)\n",
    "    # return an array of int64 numbers\n",
    "    return np.fromiter(numbers, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53011b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a graph for each \n",
    "def construct_single_graph(idx, t):\n",
    "    ''' idx: the current graph index w.r.t the label\n",
    "        t: the tree object from ete'''\n",
    "    # transform the character of amino acid in to numbers for all 5 sequences in this graph\n",
    "    transformed_x = []\n",
    "    for i in range(5):\n",
    "        # get the index of the sequence from the original dataset\n",
    "        seq_idx = 5*idx + i\n",
    "        transformed_x.append(convert_string_to_numbers(seq_string[seq_idx][:-1], dict_amino))\n",
    "        \n",
    "    # initialize the sequence of 3 internal nodes\n",
    "    vec_len = len(transformed_x[0])\n",
    "    internal_node_5 = np.full(vec_len, -1, dtype=np.int64)\n",
    "    internal_node_6 = np.full(vec_len, -1, dtype=np.int64)\n",
    "    internal_node_7 = np.full(vec_len, -1, dtype=np.int64)\n",
    "    \n",
    "    # Work out the branch distance from the Newick format\n",
    "    leaf_pair = 0               # The amount of leaf pair so far, max=2\n",
    "    prev_leaf = False           # Whether the previous leaf in the preorder is a leaf node\n",
    "    prev_dist = 0               # The distance of the branch coming out of the preivous node in preorder\n",
    "    dist_array = [0]*8          # The distance for outgoing branch for each node, node 7 will always be 0\n",
    "    prev_index = -1             # The index of the last leaf node in the preorder\n",
    "    tot_in_node = 0             # All distance of internal nodes that are unassigned so far\n",
    "    pending = False             # Some condition for assigning branch length that I don't remember\n",
    "    preorder=[]                 # The preorder of all leaf nodes\n",
    "    \n",
    "    # Traverse through all nodes in preorder, work out the branch distance \n",
    "    # There are only 2 possible rooted tree format from ETE, \n",
    "    # so 2 if statements that work out all different scenarios\n",
    "    for node in t.traverse(\"preorder\"):\n",
    "        if not node.name=='':\n",
    "            index = int(node.name) - 1\n",
    "            preorder.append(index)\n",
    "            dist_array[index] = node.dist\n",
    "            prev_index = index\n",
    "            if leaf_pair >= 2:\n",
    "                tot_in_node += node.dist\n",
    "                dist_array[index] = tot_in_node\n",
    "                break\n",
    "            else:\n",
    "                if prev_leaf==False:\n",
    "                    prev_leaf=True\n",
    "                else:\n",
    "                    leaf_pair += 1\n",
    "                    prev_leaf=False\n",
    "                    if prev_dist != 0:\n",
    "                        dist_array[leaf_pair+4] = prev_dist\n",
    "                    else:\n",
    "                        pending = True\n",
    "                    tot_in_node-=prev_dist\n",
    "        else:\n",
    "            prev_dist = node.dist\n",
    "            tot_in_node+=node.dist\n",
    "            if pending==True:\n",
    "                pending = False\n",
    "                prev_dist = 0\n",
    "                tot_in_node -= node.dist\n",
    "                dist_array[leaf_pair+4] = node.dist\n",
    "            if prev_leaf==True:\n",
    "                dist_array[prev_index] += node.dist\n",
    "                prev_dist = 0\n",
    "                tot_in_node -= node.dist\n",
    "    # Set up the adjency Matrix in COO format\n",
    "    # We find the smaller node number of each side.\n",
    "    # In this case, the tip with the larger node number is on the left side, thus connect to node 5\n",
    "    if min(preorder[0], preorder[1]) > min(preorder[2], preorder[3]):\n",
    "        # change edge value of edge 5 and 6\n",
    "        # I think this is due to the conditions from the previous part, but I don't remember the details\n",
    "        # It works though!\n",
    "        tmp = dist_array[5]\n",
    "        dist_array[5] = dist_array[6]\n",
    "        dist_array[6] = tmp\n",
    "        # Assign edge origin/destination and value\n",
    "        edge_index = torch.tensor([[preorder[2],5],[5,preorder[2]],[preorder[3],5],[5,preorder[3]],\n",
    "                                       [5,7],[7,5],[preorder[4],7],[7,preorder[4]],\n",
    "                                       [6,7],[7,6],[preorder[0],6],[6,preorder[0]],\n",
    "                                       [preorder[1],6],[6,preorder[1]]], dtype=torch.long)\n",
    "        edge_attr = [dist_array[preorder[2]], dist_array[preorder[2]], \n",
    "                 dist_array[preorder[3]], dist_array[preorder[3]], \n",
    "                 dist_array[5], dist_array[5],\n",
    "                 dist_array[preorder[4]], dist_array[preorder[4]],\n",
    "                 dist_array[6], dist_array[6],\n",
    "                 dist_array[preorder[0]], dist_array[preorder[0]],\n",
    "                 dist_array[preorder[1]], dist_array[preorder[1]]]\n",
    "        # Assign the value for internal node 5 and 6, based on the 2 leaf node they are connected with\n",
    "        for j in range(0,vec_len):\n",
    "            internal_node_5[j] = random.choice([transformed_x[preorder[2]][j],transformed_x[preorder[3]][j]])\n",
    "            internal_node_6[j] = random.choice([transformed_x[preorder[0]][j],transformed_x[preorder[1]][j]])\n",
    "    # Same thing, but now the smaller node number is on the left, thus connected with node 5\n",
    "    else:\n",
    "        edge_index = torch.tensor([[preorder[0],5],[5,preorder[0]],[preorder[1],5],[5,preorder[1]],\n",
    "                                       [5,7],[7,5],[preorder[4],7],[7,preorder[4]],\n",
    "                                       [6,7],[7,6],[preorder[2],6],[6,preorder[2]],\n",
    "                                       [preorder[3],6],[6,preorder[3]]], dtype=torch.long)\n",
    "        edge_attr = [dist_array[preorder[0]], dist_array[preorder[0]], \n",
    "                 dist_array[preorder[1]], dist_array[preorder[1]], \n",
    "                 dist_array[5], dist_array[5],\n",
    "                 dist_array[preorder[4]], dist_array[preorder[4]],\n",
    "                 dist_array[6], dist_array[6],\n",
    "                 dist_array[preorder[2]], dist_array[preorder[2]],\n",
    "                 dist_array[preorder[3]], dist_array[preorder[3]]]\n",
    "        for j in range(0,vec_len):\n",
    "            internal_node_5[j] = random.choice([transformed_x[preorder[0]][j],transformed_x[preorder[1]][j]])\n",
    "            internal_node_6[j] = random.choice([transformed_x[preorder[2]][j],transformed_x[preorder[3]][j]])\n",
    "    # Assign value for internal node 7, based on internal node 5&6, and leaf node 4\n",
    "    for j in range(0,vec_len):\n",
    "        internal_node_7[j] = random.choice([internal_node_5[j], internal_node_6[j], \n",
    "                                           transformed_x[preorder[4]][j]])\n",
    "    # append all node feature into an array\n",
    "    transformed_x.append(internal_node_5)\n",
    "    transformed_x.append(internal_node_6)\n",
    "    transformed_x.append(internal_node_7)\n",
    "    concat_x = np.array( transformed_x )\n",
    "    # create the node feature vector\n",
    "    x = torch.tensor(concat_x, dtype=torch.float)\n",
    "    # Now we create the graph object as Data\n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous(), edge_attr = torch.FloatTensor(edge_attr))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35f2d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f51b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the correct adjacency matrix of one graph\n",
    "def select_graph_original(graph_id, batch_targets, batch_index):\n",
    "    # create a true/false mask to select the graph we want from the all dense adj matrix of the whole batch\n",
    "    graph_mask = torch.eq(batch_index, graph_id)\n",
    "    graph_targets = batch_targets[graph_mask][:, graph_mask]\n",
    "    triu_indices = torch.triu_indices(graph_targets.shape[0], graph_targets.shape[0], offset=1)\n",
    "    triu_mask = torch.squeeze(to_dense_adj(triu_indices)).bool()\n",
    "    return graph_targets[triu_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6c2ed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the predicted adjacency matrix of one graph\n",
    "def select_graph_prediction(triu_logit, graph_size, start):\n",
    "    graph_triu_logit = torch.squeeze(triu_logit[start:start + graph_size])\n",
    "    return graph_triu_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d4737a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss(mu=None, logstd=None):\n",
    "    MAX_LOGSTD = 10\n",
    "    logstd =  logstd.clamp(max=MAX_LOGSTD)\n",
    "    kl_div = -0.5 * torch.mean(torch.sum(1 + 2 * logstd - mu**2 - logstd.exp()**2, dim=1))\n",
    "\n",
    "    # Limit numeric errors\n",
    "    kl_div = kl_div.clamp(max=1000)\n",
    "    return kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dca5e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(triu_logits, edge_index, mu, logvar, batch_index, kl_beta):\n",
    "    # Convert target edge index to dense adjacency matrix\n",
    "    # This is the actual adj matrix for the whole batch, converted directly from edge index\n",
    "    batch_targets = torch.squeeze(to_dense_adj(edge_index))\n",
    "    batch_recon_loss = []         # the loss for each graph in this batch\n",
    "    batch_node_counter = 0        # track which node are we in the current batch\n",
    "    for graph_id in torch.unique(batch_index):\n",
    "        # get the actual upper triangular adjacency matrix for this graph\n",
    "        graph_true_triu = select_graph_original(graph_id, batch_targets, batch_index)\n",
    "        # get the prediction of adjac matrix for this graph\n",
    "        graph_predict_triu =  select_graph_prediction(triu_logits, graph_true_triu.shape[0], batch_node_counter)\n",
    "        # update node counter\n",
    "        batch_node_counter = batch_node_counter + graph_true_triu.shape[0]\n",
    "        # Calculate edge-weighted binary cross entropy\n",
    "        weight = graph_true_triu.shape[0]/sum(graph_true_triu)\n",
    "        bce = torch.nn.BCEWithLogitsLoss(pos_weight=weight).to(device)\n",
    "        graph_recon_loss = bce(graph_predict_triu.view(-1), graph_true_triu.view(-1))\n",
    "        batch_recon_loss.append(graph_recon_loss)\n",
    "        \n",
    "    # Take average of all losses\n",
    "    num_graphs = torch.unique(batch_index).shape[0]\n",
    "    batch_recon_loss = sum(batch_recon_loss) / num_graphs\n",
    "    # KL Divergence\n",
    "    kl_divergence = kl_loss(mu, logvar)\n",
    "    return batch_recon_loss + kl_beta * kl_divergence, kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa8da38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_triu_graph_reconstruction(graph_predictions_triu, graph_targets_triu, num_nodes=None):\n",
    "    # Apply sigmoid to get binary prediction values\n",
    "    preds = (torch.sigmoid(graph_predictions_triu.view(-1)) > 0.5).int()\n",
    "    # Reshape the ground truth\n",
    "    labels = graph_targets_triu.view(-1)\n",
    "     # Check if the predictions and the groundtruth match\n",
    "    if labels.shape[0] == sum(torch.eq(preds, labels)):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b91dc561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the accuracy for each epoch (both train and test)\n",
    "def reconstruction_accuracy(triu_logits, edge_index, batch_index):\n",
    "    batch_targets = torch.squeeze(to_dense_adj(edge_index))\n",
    "    batch_targets_triu = []\n",
    "    # Iterate over batch and collect each of the trius\n",
    "    batch_node_counter = 0\n",
    "    num_recon = 0\n",
    "    for graph_id in torch.unique(batch_index):\n",
    "        # Get triu parts for this graph\n",
    "        graph_targets_triu = select_graph_original(graph_id, \n",
    "                                                batch_targets, \n",
    "                                                batch_index)\n",
    "        graph_predictions_triu = select_graph_prediction(triu_logits, \n",
    "                                                        graph_targets_triu.shape[0], \n",
    "                                                        batch_node_counter)\n",
    "        # Update counter to the index of the next graph\n",
    "        batch_node_counter = batch_node_counter + graph_targets_triu.shape[0]\n",
    "        # Check if graph is successfully reconstructed\n",
    "        num_nodes = sum(torch.eq(batch_index, graph_id))\n",
    "        recon = check_triu_graph_reconstruction(graph_predictions_triu, \n",
    "                                                graph_targets_triu, \n",
    "                                                num_nodes) \n",
    "        num_recon = num_recon + int(recon)\n",
    "        batch_targets_triu.append(graph_targets_triu)\n",
    "    \n",
    "    batch_targets_triu = torch.cat(batch_targets_triu)\n",
    "    triu_discrete = torch.squeeze(torch.tensor(torch.sigmoid(triu_logits) > 0.5, dtype=torch.int32))\n",
    "    acc = torch.true_divide(torch.sum(batch_targets_triu==triu_discrete), batch_targets_triu.shape[0]) \n",
    "        \n",
    "    return acc.detach().cpu().numpy(), num_recon    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "150d3627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_epoch(data_loader, type, epoch, kl_beta):\n",
    "    all_losses = []\n",
    "    all_accs = []\n",
    "    all_kldivs = []\n",
    "    \n",
    "    reconstructed_tree = 0\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(data_loader)):\n",
    "        try:\n",
    "            batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            triu_logits, mu, logvar = model(batch.x.float(), batch.edge_index, batch.batch)\n",
    "            loss, kl_loss = loss_func(triu_logits, batch.edge_index, mu, logvar, batch.batch, kl_beta)\n",
    "            if type == \"Train\":\n",
    "                loss.backward()  \n",
    "                optimizer.step()  \n",
    "            # Calculate metrics\n",
    "            acc, num_recon = reconstruction_accuracy(triu_logits, batch.edge_index, batch.batch)\n",
    "            reconstructed_tree = reconstructed_tree + num_recon\n",
    "            \n",
    "            all_losses.append(loss.detach().cpu().numpy())\n",
    "            all_accs.append(acc)\n",
    "            all_kldivs.append(kl_loss.detach().cpu().numpy())\n",
    "        except IndexError as error:\n",
    "            print(\"Error: \", error)\n",
    "    \n",
    "    print(f\"{type} epoch {epoch} loss: \", np.array(all_losses).mean())\n",
    "    print(f\"{type} epoch {epoch} accuracy: \", np.array(all_accs).mean())\n",
    "    print(f\"Reconstructed {reconstructed_tree}.\")\n",
    "    mlflow.log_metric(key=f\"{type} Epoch Loss\", value=float(np.array(all_losses).mean()), step=epoch)\n",
    "    mlflow.log_metric(key=f\"{type} Epoch Accuracy\", value=float(np.array(all_accs).mean()), step=epoch)\n",
    "    mlflow.log_metric(key=f\"{type} Num Reconstructed\", value=float(reconstructed_tree), step=epoch)\n",
    "    mlflow.log_metric(key=f\"{type} KL Divergence\", value=float(np.array(all_kldivs).mean()), step=epoch)\n",
    "    #mlflow.log_model(model, \"model\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ace10d2",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8e8a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GVAE(nn.Module):\n",
    "    def __init__(self, feature_size, embedding_size, edge_dim):\n",
    "        super(GVAE, self).__init__()\n",
    "        self.latent_embedding_size = int(embedding_size/2)\n",
    "        decoder_size = embedding_size*4\n",
    "        \n",
    "        # Encoder Layers\n",
    "        # 3 layers with batch normalization\n",
    "        self.conv1 = TransformerConv(feature_size,\n",
    "                                    embedding_size*4,\n",
    "                                    heads=4,\n",
    "                                    concat=False,\n",
    "                                    beta=True)\n",
    "                                            #edge_dim=edge_dim)\n",
    "        self.bn1 = BatchNorm(embedding_size*4)\n",
    "        self.conv2 = TransformerConv(embedding_size*4,\n",
    "                                    embedding_size*2,\n",
    "                                    heads=4,\n",
    "                                    concat=False,\n",
    "                                    beta=True)\n",
    "                                            #edge_dim=edge_dim)\n",
    "        self.bn2 = BatchNorm(embedding_size*2)\n",
    "        self.conv3 = TransformerConv(embedding_size*2,\n",
    "                                    embedding_size,\n",
    "                                    heads=4,\n",
    "                                    concat=False,\n",
    "                                    beta=True)\n",
    "                                            #edge_dim=edge_dim)\n",
    "        self.bn3 = BatchNorm(embedding_size)\n",
    "        \n",
    "        # Latent transform\n",
    "        self.mu_transform = TransformerConv(embedding_size, \n",
    "                                            self.latent_embedding_size,\n",
    "                                            heads=4,\n",
    "                                            concat=False,\n",
    "                                            beta=True)\n",
    "                                            #edge_dim=edge_dim)\n",
    "        self.logvar_transform = TransformerConv(embedding_size, \n",
    "                                            self.latent_embedding_size,\n",
    "                                            heads=4,\n",
    "                                            concat=False,\n",
    "                                            beta=True)\n",
    "                                            #edge_dim=edge_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_dense_1 = Linear(self.latent_embedding_size*2, decoder_size)\n",
    "        self.decoder_bn_1 = BatchNorm1d(decoder_size)\n",
    "        self.decoder_dense_2 = Linear(self.latent_embedding_size*2, decoder_size)\n",
    "        self.decoder_bn_2 = BatchNorm1d(decoder_size)\n",
    "        self.decoder_dense_3 = Linear(self.latent_embedding_size*2, decoder_size)\n",
    "        self.decoder_bn_3 = BatchNorm1d(decoder_size)\n",
    "        self.decoder_dense_4 = Linear(decoder_size, 1)\n",
    "    \n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.bn2(x)\n",
    "        x = self.conv3(x, edge_index).relu()\n",
    "        x = self.bn3(x)\n",
    "        # latent variable\n",
    "        mu = self.mu_transform(x, edge_index)\n",
    "        logvar = self.logvar_transform(x, edge_index)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def decode(self, z, batch_index):\n",
    "        # z: the 5 latent vectors for all graph\n",
    "        # batch\n",
    "        inputs = []\n",
    "        \n",
    "        # for each graph in this batch\n",
    "        for graph_id in torch.unique(batch_index):\n",
    "            # Select the latent vectors for the graphs in this batch\n",
    "            graph_mask = torch.eq(batch_index, graph_id)\n",
    "            graph_z = z[graph_mask]\n",
    "            # Get upper triangle adjacency matrix\n",
    "            # the diagonal is not included (the node is always connected to itself)\n",
    "            # should be something like this:\n",
    "            # [0,0,0,0,0,0,0,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,4,4,4,5,5,6],\n",
    "            # [1,2,3,4,5,6,7,2,3,4,5,6,7,3,4,5,6,7,4,5,6,7,5,6,7,6,7,7]\n",
    "            # Note that each column is a pair of possible connection. Now we have all possible connection\n",
    "            edge_indices = torch.triu_indices(graph_z.shape[0], graph_z.shape[0], offset=1)\n",
    "            # We want to put actual latent vectors in the place.\n",
    "            # i.e, replace 0 in the previous array with latent vector for node 0.\n",
    "            dim = self.latent_embedding_size\n",
    "            # create the shape for source and target\n",
    "            # each should be a 28*dim (length of the edge_indices[0]) array with same values for each row indicating the node number\n",
    "            source_indices = torch.reshape(edge_indices[0].repeat_interleave(dim), (edge_indices.shape[1], dim))\n",
    "            target_indices = torch.reshape(edge_indices[1].repeat_interleave(dim), (edge_indices.shape[1], dim))\n",
    "            # Get the latent vectors\n",
    "            # should fill the previous arrays with actual latent vectors such as \n",
    "            # [-1, 0, 3.2, ..., -2.1, 0] (latent vector for node 0)\n",
    "            # [-1, 0, 3.2, ..., -2.1, 0]\n",
    "            #        ......              (28 rows)\n",
    "            # [0.3, 4.2, ..., 8, 2.5, 5] (latent vector for node 6)\n",
    "            sources_latent = torch.gather(graph_z, 0, source_indices.to(device))\n",
    "            target_latent = torch.gather(graph_z, 0, target_indices.to(device))\n",
    "            # Should be 28(pairs of node) * (2*dim)\n",
    "            graph_inputs = torch.cat([sources_latent, target_latent], axis=1)\n",
    "            inputs.append(graph_inputs)\n",
    "        \n",
    "        # now we concat all graphs in this batch\n",
    "        inputs = torch.cat(inputs)\n",
    "        # feed into the decoding layers\n",
    "        x = self.decoder_dense_1(inputs).relu()\n",
    "        x = self.decoder_bn_1(x)\n",
    "        x = self.decoder_dense_2(inputs).relu()\n",
    "        x = self.decoder_bn_2(x)\n",
    "        x = self.decoder_dense_3(inputs).relu()\n",
    "        x = self.decoder_bn_3(x)\n",
    "        edge_logits = self.decoder_dense_4(x)\n",
    "        \n",
    "        # We transform the logits later for probabilities\n",
    "        return edge_logits\n",
    "    \n",
    "    # transform mu and logvar into the latent vectors\n",
    "    def reparam(self, mu, logvar):\n",
    "        if self.training:\n",
    "            # transform logvar\n",
    "            std = torch.exp(logvar)\n",
    "            # generate same amount of random numbers from N(0, 1)\n",
    "            eps = torch.randn_like(std)\n",
    "            # get the sampled value\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "    def forward(self, x, edge_index, batch_index):\n",
    "        # GNN layers\n",
    "        mu, logvar = self.encode(x, edge_index)\n",
    "        # latent vectors\n",
    "        z = self.reparam(mu, logvar)\n",
    "        # Decode layers\n",
    "        logit = self.decode(z, batch_index)\n",
    "        \n",
    "        return logit, mu, logvar\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad071d",
   "metadata": {},
   "source": [
    "# File inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f2eb7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training the Garph Auto Encoder for 5-taxa dataset\n",
      "------------------------------------------------------------------------\n",
      "Executing gae_model.py following gae.json\n",
      "------------------------------------------------------------------------\n",
      "Loading Sequence Data in sequences12062021.in\n",
      "Loading Label Data in labels12062021.in\n",
      "Loading Tree Data in trees12062021.in\n",
      "Number of samples:10000; Sequence length of each sample:1550\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# get name of the script\n",
    "# nameScript = sys.argv[0].split('/')[-1]\n",
    "nameScript = \"gae_model.py\"\n",
    "# get json file name of the script\n",
    "nameJson = \"gae.json\"\n",
    "# nameJson = sys.argv[1]\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(\"Training the Garph Auto Encoder for 5-taxa dataset\")\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(\"Executing \" + nameScript + \" following \" + nameJson, flush = True)\n",
    "\n",
    "# opening Json file \n",
    "jsonFile = open(nameJson) \n",
    "dataJson = json.load(jsonFile)\n",
    "\n",
    "# loading the input data from the json file\n",
    "ngpu = dataJson[\"ngpu\"]                  # number of GPUS\n",
    "lr = dataJson[\"lr\"]                      # learning rate\n",
    "embedSize = dataJson[\"embedSize\"]        # Embedding size\n",
    "nEpochs = dataJson[\"nEpochs\"]            # Number of Epochs\n",
    "batchSize = dataJson[\"batchSize\"]        # batchSize\n",
    "kl_beta = dataJson[\"klBeta\"]\n",
    "\n",
    "data_root = dataJson[\"dataRoot\"]         # data folder\n",
    "model_root = dataJson[\"modelRoot\"]       # folder to save the data\n",
    "\n",
    "label_files = dataJson[\"labelFile\"]      # file with labels\n",
    "sequence_files = dataJson[\"matFile\"]     # file with sequences\n",
    "tree_files = dataJson[\"treeFile\"]        # file with tree structure\n",
    "\n",
    "if \"summaryFile\" in dataJson:\n",
    "    summary_file = dataJson[\"summaryFile\"]\n",
    "else :\n",
    "    summary_file = \"summary_file.txt\"\n",
    "\n",
    "\n",
    "print(\"------------------------------------------------------------------------\") \n",
    "print(\"Loading Sequence Data in \" + sequence_files, flush = True)\n",
    "print(\"Loading Label Data in \" + label_files, flush = True)\n",
    "print(\"Loading Tree Data in \" + tree_files, flush = True)\n",
    "\n",
    "# we read the labels as list of strings\n",
    "with open(data_root+label_files, 'r') as f:\n",
    "    label_char = f.readlines()\n",
    "\n",
    "# we read the sequence as a list of strings\n",
    "with open(data_root+sequence_files, 'r') as f:\n",
    "    seq_string = f.readlines()\n",
    "\n",
    "with open(data_root+tree_files, 'r') as f:\n",
    "    tree_newick = f.readlines()\n",
    "    \n",
    "n_samples = len(label_char)\n",
    "seq_length = len(seq_string[0])-1\n",
    "print(\"Number of samples:{}; Sequence length of each sample:{}\"\n",
    "        .format(n_samples, seq_length))\n",
    "print(\"------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a06525",
   "metadata": {},
   "source": [
    "# Data pre-processing\n",
    "Read Sequence data and Newick tree format, return the all graph object with necessary info in the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e05e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to extract the dictionary with the relative positions\n",
    "# for each aminoacid\n",
    "\n",
    "# first we need to extract all the different chars\n",
    "strL = \"\"\n",
    "for c in seq_string[0][:-1]:\n",
    "    if not c in strL:\n",
    "        strL += c\n",
    "\n",
    "# we sort them\n",
    "strL = sorted(strL)\n",
    "\n",
    "# we give them a relative order\n",
    "dict_amino = {}\n",
    "for ii, c in enumerate(strL):\n",
    "    dict_amino[c] = ii\n",
    "\n",
    "# looping over the labels and create array. Here each element of the\n",
    "# label_char has the form \"1\\n\", so we only take the first one\n",
    "labels = np.fromiter(map(lambda x: int(x[0])-1,\n",
    "                         label_char), dtype= np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "766ca993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all graphs from raw dataset\n",
    "# EXTREMELY SLOW\n",
    "dataset = []  # empty dataset for all graphs\n",
    "# loop through all samples\n",
    "for i in range(n_samples):\n",
    "    # Get the ete tree format\n",
    "    tree = tree_newick[i][:-1]\n",
    "    t = Tree(tree)\n",
    "    # get node feature, COO adjacency matrix, and edge feature\n",
    "    data = construct_single_graph(i, t)\n",
    "    # Validate if number of node and edges match\n",
    "    if (not data.validate(raise_on_error=True)):\n",
    "        print(\"Error! Node number and edge set does not match!\")\n",
    "        break\n",
    "    # Add the graph into the dataset\n",
    "    dataset.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a9d0c2",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2994f4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start the training process...\n",
      "------------------------------------------------------------------------\n",
      "Data loaded, loading model...\n",
      "Number of parameters in the model:  7454561\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/282 [00:00<?, ?it/s]C:\\Users\\Xudong\\AppData\\Local\\Temp\\ipykernel_3196\\3221043966.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  triu_discrete = torch.squeeze(torch.tensor(torch.sigmoid(triu_logits) > 0.5, dtype=torch.int32))\n",
      "100%|██████████| 282/282 [02:20<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 0 loss:  0.9727064\n",
      "Train epoch 0 accuracy:  0.53643465\n",
      "Reconstructed 189.\n",
      "Evluating testset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:13<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test epoch 0 loss:  0.46409982\n",
      "Test epoch 0 accuracy:  0.8544574\n",
      "Reconstructed 146.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [02:21<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 1 loss:  0.31166732\n",
      "Train epoch 1 accuracy:  0.9269132\n",
      "Reconstructed 3379.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [02:18<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 2 loss:  0.19313523\n",
      "Train epoch 2 accuracy:  0.9643293\n",
      "Reconstructed 5560.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [02:19<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 3 loss:  0.17779247\n",
      "Train epoch 3 accuracy:  0.96929616\n",
      "Reconstructed 5939.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [02:21<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 4 loss:  0.1573883\n",
      "Train epoch 4 accuracy:  0.9759965\n",
      "Reconstructed 6455.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [02:25<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 5 loss:  0.14164266\n",
      "Train epoch 5 accuracy:  0.9800414\n",
      "Reconstructed 6781.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [02:24<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 6 loss:  0.12659729\n",
      "Train epoch 6 accuracy:  0.9830333\n",
      "Reconstructed 7126.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [02:23<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 7 loss:  0.11700361\n",
      "Train epoch 7 accuracy:  0.9854358\n",
      "Reconstructed 7331.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [02:21<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 8 loss:  0.115801044\n",
      "Train epoch 8 accuracy:  0.98605305\n",
      "Reconstructed 7477.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [02:23<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 9 loss:  0.09957878\n",
      "Train epoch 9 accuracy:  0.98881954\n",
      "Reconstructed 7701.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [02:21<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 10 loss:  0.09816704\n",
      "Train epoch 10 accuracy:  0.98980105\n",
      "Reconstructed 7801.\n",
      "Evluating testset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:14<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test epoch 10 loss:  0.08694739\n",
      "Test epoch 10 accuracy:  0.989781\n",
      "Reconstructed 903.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [02:23<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 11 loss:  0.08275391\n",
      "Train epoch 11 accuracy:  0.9915622\n",
      "Reconstructed 7996.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [02:19<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 12 loss:  0.069262385\n",
      "Train epoch 12 accuracy:  0.99407136\n",
      "Reconstructed 8257.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [02:20<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 13 loss:  0.065354675\n",
      "Train epoch 13 accuracy:  0.9948274\n",
      "Reconstructed 8318.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [02:23<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 14 loss:  0.062039368\n",
      "Train epoch 14 accuracy:  0.99529433\n",
      "Reconstructed 8399.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [02:25<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 15 loss:  0.07219703\n",
      "Train epoch 15 accuracy:  0.99326795\n",
      "Reconstructed 8150.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [02:22<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 16 loss:  0.056797087\n",
      "Train epoch 16 accuracy:  0.9957732\n",
      "Reconstructed 8427.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [02:25<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 17 loss:  0.05620309\n",
      "Train epoch 17 accuracy:  0.9957732\n",
      "Reconstructed 8455.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [02:23<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 18 loss:  0.054022375\n",
      "Train epoch 18 accuracy:  0.995971\n",
      "Reconstructed 8462.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 66/282 [00:34<01:49,  1.97it/s]"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Start the training process...\")\n",
    "train_dataset = dataset[:9000]\n",
    "test_dataset = dataset[9000:]\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchSize, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batchSize, shuffle=True)\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(\"Data loaded, loading model...\")\n",
    "# Load model\n",
    "model = GVAE(seq_length, embedSize, 1)\n",
    "print(\"Number of parameters in the model: \", count_parameters(model))\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    for epoch in range(nEpochs): \n",
    "        model.train()\n",
    "        run_one_epoch(train_loader, type=\"Train\", epoch=epoch, kl_beta=kl_beta)\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Evluating testset...\")\n",
    "            model.eval()\n",
    "            run_one_epoch(test_loader, type=\"Test\", epoch=epoch, kl_beta=kl_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a88ed33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1254ddbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
