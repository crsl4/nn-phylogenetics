{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea66606",
   "metadata": {},
   "source": [
    "# Import Packages and environmental setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6f1490b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import ete3\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "import itertools\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from os import path\n",
    "sys.path.insert(0, '../')\n",
    "import gc\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn.conv import TransformerConv\n",
    "from torch_geometric.nn import VGAE\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import batched_negative_sampling\n",
    "from ete3 import Tree\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcfa2b8",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d632127c",
   "metadata": {},
   "source": [
    "The internal node will be in the order of 5-7-6, the single tip will always be connected to node 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26c885d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert string to numbers\n",
    "def convert_string_to_numbers(str, dict):\n",
    "    ''' str: string to convert\n",
    "        dict dictionary with the relative ordering of each char'''\n",
    "            # create a map iterator using a lambda function\n",
    "    # lambda x -> return dict[x]\n",
    "    # This return the value for each key in dict based on str\n",
    "    numbers = map(lambda x: dict[x], str)\n",
    "    # return an array of int64 numbers\n",
    "    return np.fromiter(numbers, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53011b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a graph for each \n",
    "def construct_single_graph(idx, t):\n",
    "    ''' idx: the current graph index w.r.t the label\n",
    "        t: the tree object from ete'''\n",
    "    # transform the character of amino acid in to numbers for all 5 sequences in this graph\n",
    "    transformed_x = []\n",
    "    for i in range(5):\n",
    "        # get the index of the sequence from the original dataset\n",
    "        seq_idx = 5*idx + i\n",
    "        transformed_x.append(convert_string_to_numbers(seq_string[seq_idx][:-1], dict_amino))\n",
    "        \n",
    "    # initialize the sequence of 3 internal nodes\n",
    "    vec_len = len(transformed_x[0])\n",
    "    internal_node_5 = np.full(vec_len, -1, dtype=np.int64)\n",
    "    internal_node_6 = np.full(vec_len, -1, dtype=np.int64)\n",
    "    internal_node_7 = np.full(vec_len, -1, dtype=np.int64)\n",
    "    \n",
    "    # Work out the branch distance from the Newick format\n",
    "    leaf_pair = 0               # The amount of leaf pair so far, max=2\n",
    "    prev_leaf = False           # Whether the previous leaf in the preorder is a leaf node\n",
    "    prev_dist = 0               # The distance of the branch coming out of the preivous node in preorder\n",
    "    dist_array = [0]*8          # The distance for outgoing branch for each node, node 7 will always be 0\n",
    "    prev_index = -1             # The index of the last leaf node in the preorder\n",
    "    tot_in_node = 0             # All distance of internal nodes that are unassigned so far\n",
    "    pending = False             # Some condition for assigning branch length that I don't remember\n",
    "    preorder=[]                 # The preorder of all leaf nodes\n",
    "    \n",
    "    # Traverse through all nodes in preorder, work out the branch distance \n",
    "    # There are only 2 possible rooted tree format from ETE, \n",
    "    # so 2 if statements that work out all different scenarios\n",
    "    for node in t.traverse(\"preorder\"):\n",
    "        if not node.name=='':\n",
    "            index = int(node.name) - 1\n",
    "            preorder.append(index)\n",
    "            dist_array[index] = node.dist\n",
    "            prev_index = index\n",
    "            if leaf_pair >= 2:\n",
    "                tot_in_node += node.dist\n",
    "                dist_array[index] = tot_in_node\n",
    "                break\n",
    "            else:\n",
    "                if prev_leaf==False:\n",
    "                    prev_leaf=True\n",
    "                else:\n",
    "                    leaf_pair += 1\n",
    "                    prev_leaf=False\n",
    "                    if prev_dist != 0:\n",
    "                        dist_array[leaf_pair+4] = prev_dist\n",
    "                    else:\n",
    "                        pending = True\n",
    "                    tot_in_node-=prev_dist\n",
    "        else:\n",
    "            prev_dist = node.dist\n",
    "            tot_in_node+=node.dist\n",
    "            if pending==True:\n",
    "                pending = False\n",
    "                prev_dist = 0\n",
    "                tot_in_node -= node.dist\n",
    "                dist_array[leaf_pair+4] = node.dist\n",
    "            if prev_leaf==True:\n",
    "                dist_array[prev_index] += node.dist\n",
    "                prev_dist = 0\n",
    "                tot_in_node -= node.dist\n",
    "    # Set up the adjency Matrix in COO format\n",
    "    # We find the smaller node number of each side.\n",
    "    # In this case, the tip with the larger node number is on the left side, thus connect to node 5\n",
    "    if min(preorder[0], preorder[1]) > min(preorder[2], preorder[3]):\n",
    "        # change edge value of edge 5 and 6\n",
    "        # I think this is due to the conditions from the previous part, but I don't remember the details\n",
    "        # It works though!\n",
    "        tmp = dist_array[5]\n",
    "        dist_array[5] = dist_array[6]\n",
    "        dist_array[6] = tmp\n",
    "        # Assign edge origin/destination and value\n",
    "        edge_index = torch.tensor([[preorder[2],5],[5,preorder[2]],[preorder[3],5],[5,preorder[3]],\n",
    "                                       [5,7],[7,5],[preorder[4],7],[7,preorder[4]],\n",
    "                                       [6,7],[7,6],[preorder[0],6],[6,preorder[0]],\n",
    "                                       [preorder[1],6],[6,preorder[1]]], dtype=torch.long)\n",
    "        edge_attr = [dist_array[preorder[2]], dist_array[preorder[2]], \n",
    "                 dist_array[preorder[3]], dist_array[preorder[3]], \n",
    "                 dist_array[5], dist_array[5],\n",
    "                 dist_array[preorder[4]], dist_array[preorder[4]],\n",
    "                 dist_array[6], dist_array[6],\n",
    "                 dist_array[preorder[0]], dist_array[preorder[0]],\n",
    "                 dist_array[preorder[1]], dist_array[preorder[1]]]\n",
    "        # Assign the value for internal node 5 and 6, based on the 2 leaf node they are connected with\n",
    "        for j in range(0,vec_len):\n",
    "            internal_node_5[j] = random.choice([transformed_x[preorder[2]][j],transformed_x[preorder[3]][j]])\n",
    "            internal_node_6[j] = random.choice([transformed_x[preorder[0]][j],transformed_x[preorder[1]][j]])\n",
    "    # Same thing, but now the smaller node number is on the left, thus connected with node 5\n",
    "    else:\n",
    "        edge_index = torch.tensor([[preorder[0],5],[5,preorder[0]],[preorder[1],5],[5,preorder[1]],\n",
    "                                       [5,7],[7,5],[preorder[4],7],[7,preorder[4]],\n",
    "                                       [6,7],[7,6],[preorder[2],6],[6,preorder[2]],\n",
    "                                       [preorder[3],6],[6,preorder[3]]], dtype=torch.long)\n",
    "        edge_attr = [dist_array[preorder[0]], dist_array[preorder[0]], \n",
    "                 dist_array[preorder[1]], dist_array[preorder[1]], \n",
    "                 dist_array[5], dist_array[5],\n",
    "                 dist_array[preorder[4]], dist_array[preorder[4]],\n",
    "                 dist_array[6], dist_array[6],\n",
    "                 dist_array[preorder[2]], dist_array[preorder[2]],\n",
    "                 dist_array[preorder[3]], dist_array[preorder[3]]]\n",
    "        for j in range(0,vec_len):\n",
    "            internal_node_5[j] = random.choice([transformed_x[preorder[0]][j],transformed_x[preorder[1]][j]])\n",
    "            internal_node_6[j] = random.choice([transformed_x[preorder[2]][j],transformed_x[preorder[3]][j]])\n",
    "    # Assign value for internal node 7, based on internal node 5&6, and leaf node 4\n",
    "    for j in range(0,vec_len):\n",
    "        internal_node_7[j] = random.choice([internal_node_5[j], internal_node_6[j], \n",
    "                                           transformed_x[preorder[4]][j]])\n",
    "    # append all node feature into an array\n",
    "    transformed_x.append(internal_node_5)\n",
    "    transformed_x.append(internal_node_6)\n",
    "    transformed_x.append(internal_node_7)\n",
    "    # create the node feature vector\n",
    "    x = torch.tensor(transformed_x, dtype=torch.float)\n",
    "    # Now we create the graph object as Data\n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous(), edge_attr = edge_attr)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad071d",
   "metadata": {},
   "source": [
    "# File inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f2eb7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training the Garph Auto Encoder for 5-taxa dataset\n",
      "------------------------------------------------------------------------\n",
      "Executing gae_model.py following gae.json\n",
      "------------------------------------------------------------------------\n",
      "Loading Sequence Data in sequences12062021.in\n",
      "Loading Label Data in labels12062021.in\n",
      "Loading Tree Data in trees12062021.in\n",
      "Number of samples:10000; Sequence length of each sample:1550\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# get name of the script\n",
    "# nameScript = sys.argv[0].split('/')[-1]\n",
    "nameScript = \"gae_model.py\"\n",
    "# get json file name of the script\n",
    "nameJson = \"gae.json\"\n",
    "# nameJson = sys.argv[1]\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(\"Training the Garph Auto Encoder for 5-taxa dataset\")\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(\"Executing \" + nameScript + \" following \" + nameJson, flush = True)\n",
    "\n",
    "# opening Json file \n",
    "jsonFile = open(nameJson) \n",
    "dataJson = json.load(jsonFile)\n",
    "\n",
    "# loading the input data from the json file\n",
    "ngpu = dataJson[\"ngpu\"]                  # number of GPUS\n",
    "lr = dataJson[\"lr\"]                      # learning rate\n",
    "embedSize = dataJson[\"embedSize\"]        # Embedding size\n",
    "nEpochs = dataJson[\"nEpochs\"]            # Number of Epochs\n",
    "batchSize = dataJson[\"batchSize\"]        # batchSize\n",
    "\n",
    "\n",
    "data_root = dataJson[\"dataRoot\"]         # data folder\n",
    "model_root = dataJson[\"modelRoot\"]       # folder to save the data\n",
    "\n",
    "label_files = dataJson[\"labelFile\"]      # file with labels\n",
    "sequence_files = dataJson[\"matFile\"]     # file with sequences\n",
    "tree_files = dataJson[\"treeFile\"]        # file with tree structure\n",
    "\n",
    "if \"summaryFile\" in dataJson:\n",
    "    summary_file = dataJson[\"summaryFile\"]\n",
    "else :\n",
    "    summary_file = \"summary_file.txt\"\n",
    "\n",
    "\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(\"Loading Sequence Data in \" + sequence_files, flush = True)\n",
    "print(\"Loading Label Data in \" + label_files, flush = True)\n",
    "print(\"Loading Tree Data in \" + tree_files, flush = True)\n",
    "\n",
    "# we read the labels as list of strings\n",
    "with open(data_root+label_files, 'r') as f:\n",
    "    label_char = f.readlines()\n",
    "\n",
    "# we read the sequence as a list of strings\n",
    "with open(data_root+sequence_files, 'r') as f:\n",
    "    seq_string = f.readlines()\n",
    "\n",
    "with open(data_root+tree_files, 'r') as f:\n",
    "    tree_newick = f.readlines()\n",
    "    \n",
    "n_samples = len(label_char)\n",
    "seq_length = len(seq_string[0])-1\n",
    "print(\"Number of samples:{}; Sequence length of each sample:{}\"\n",
    "        .format(n_samples, seq_length))\n",
    "print(\"------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a06525",
   "metadata": {},
   "source": [
    "# Data pre-processing\n",
    "Read Sequence data and Newick tree format, return the all graph object with necessary info in the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e05e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to extract the dictionary with the relative positions\n",
    "# for each aminoacid\n",
    "\n",
    "# first we need to extract all the different chars\n",
    "strL = \"\"\n",
    "for c in seq_string[0][:-1]:\n",
    "    if not c in strL:\n",
    "        strL += c\n",
    "\n",
    "# we sort them\n",
    "strL = sorted(strL)\n",
    "\n",
    "# we give them a relative order\n",
    "dict_amino = {}\n",
    "for ii, c in enumerate(strL):\n",
    "    dict_amino[c] = ii\n",
    "\n",
    "# looping over the labels and create array. Here each element of the\n",
    "# label_char has the form \"1\\n\", so we only take the first one\n",
    "labels = np.fromiter(map(lambda x: int(x[0])-1,\n",
    "                         label_char), dtype= np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "766ca993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26095/1683458778.py:115: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  x = torch.tensor(transformed_x, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "# Create all graphs from raw dataset\n",
    "\n",
    "dataset = []  # empty dataset for all graphs\n",
    "# loop through all samples\n",
    "for i in range(n_samples):\n",
    "    # Get the ete tree format\n",
    "    tree = tree_newick[i][:-1]\n",
    "    t = Tree(tree)\n",
    "    # get node feature, COO adjacency matrix, and edge feature\n",
    "    data = construct_single_graph(i, t)\n",
    "    # Validate if number of node and edges match\n",
    "    if (not data.validate(raise_on_error=True)):\n",
    "        print(\"Error! Node number and edge set does not match!\")\n",
    "        break\n",
    "    # Add the graph into the dataset\n",
    "    dataset.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2c490f",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8e10d9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (2661054014.py, line 67)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 67\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class GVAE(nn.Module):\n",
    "    def __init__(self, feature_size, embedding_size, edge_dim):\n",
    "        super(GVAE, self).__init__()\n",
    "        self.latent_embedding_size = embedding_size/2\n",
    "        decoder_size = embedding_size*4\n",
    "        \n",
    "        # Encoder Layers\n",
    "        # 3 layers with batch normalization\n",
    "        self.conv1 = TransformerConv(feature_size,\n",
    "                                    embedding_size*6,\n",
    "                                    heads=4,\n",
    "                                    concat=False,\n",
    "                                    beta=True,\n",
    "                                    edge_dim=edge_dim)\n",
    "        self.bn1 = BatchNorm(embedding_size*6)\n",
    "        self.conv2 = TransformerConv(embedding_size*6,\n",
    "                                    embedding_size*3,\n",
    "                                    heads=4,\n",
    "                                    concat=False,\n",
    "                                    beta=True,\n",
    "                                    edge_dim=edge_dim)\n",
    "        self.bn2 = BatchNorm(embedding_size*3)\n",
    "        self.conv3 = TransformerConv(embedding_size*3,\n",
    "                                    embedding_size,\n",
    "                                    heads=4,\n",
    "                                    concat=False,\n",
    "                                    beta=True,\n",
    "                                    edge_dim=edge_dim)\n",
    "        self.bn3 = BatchNorm(embedding_size)\n",
    "        \n",
    "        # Latent transform\n",
    "        self.mu_transform = TransformerConv(embedding_size, \n",
    "                                            self.latent_embedding_size,\n",
    "                                            heads=4,\n",
    "                                            concat=False,\n",
    "                                            beta=True,\n",
    "                                            edge_dim=edge_dim)\n",
    "        self.logvar_transform = TransformerConv(embedding_size, \n",
    "                                            self.latent_embedding_size,\n",
    "                                            heads=4,\n",
    "                                            concat=False,\n",
    "                                            beta=True,\n",
    "                                            edge_dim=edge_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_dense_1 = Linear(self.latent_embedding_size*2, decoder_size)\n",
    "        self.decoder_bn_1 = BatchNorm1d(decoder_size)\n",
    "        self.decoder_dense_2 = Linear(self.latent_embedding_size*2, decoder_size)\n",
    "        self.decoder_bn_2 = BatchNorm1d(decoder_size)\n",
    "        self.decoder_dense_3 = Linear(self.latent_embedding_size*2, decoder_size)\n",
    "        self.decoder_bn_3 = BatchNorm1d(decoder_size)\n",
    "        self.decoder_dense_4 = Linear(decoder_size, 1)\n",
    "    \n",
    "    def encode(self, x, edge_index, edge_attr):\n",
    "        x = self.conv1(x, edge_index, edge_attr).relu()\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr).relu()\n",
    "        x = self.bn2(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr).relu()\n",
    "        x = self.bn3(x)\n",
    "        # latent variable\n",
    "        mu = self.mu_transform(x, edge_index, edge_attr)\n",
    "        logvar = self.logvar_transform(x, edge_index, edge_attr)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def decode(self, z, batch_index):\n",
    "        # z: the 5 latent vectors for all graph\n",
    "        # batch\n",
    "        inputs = []\n",
    "        \n",
    "        for graph_id in torch.unique(batch_index):\n",
    "            # Select the latent vectors for the graphs in this batch\n",
    "            graph_mask = torch.eq(batch_index, graph_id)\n",
    "            graph_z = z[graph_mask]\n",
    "            # Get upper triangle adjacency matrix\n",
    "            edge_indices = torch.triu_indices(graph_z.shape[0], graph_z.shape[0], offset=1)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c172949d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53a9d0c2",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2994f4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[:9000]\n",
    "test_dataset = dataset[9000:]\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchSize, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batchSize, shuffle=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8a88ed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "def run_one_epoch(data_loader):\n",
    "    for _, batch in enumerate(tqdm(data_loader)):\n",
    "        batch.to(device) \n",
    "        optimizer.zero_grad() \n",
    "        batch_neg_edge = batched_negative_sampling(batch.edge_index, batch.batch)\n",
    "        z = model.encode(batch.x, batch.edge_index)\n",
    "        loss = model.recon_loss(z, batch.edge_index, batch_neg_edge)\n",
    "        loss = loss + (1 / batch.num_nodes) * model.kl_loss()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "#def test(data_loader):\n",
    "#    for _, batch in enumerate(tqdm(data_loader)):\n",
    "#        with torch.no_grad():\n",
    "#            z = model.encode(x, train_pos_edge_index)\n",
    "#    return model.test(z, pos_edge_index, neg_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1254ddbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4a0cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85c238f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8165090208152743,\n",
       " 0.8165090208152743,\n",
       " 0.3402578285913327,\n",
       " 0.3402578285913327,\n",
       " 0.3584708485721173,\n",
       " 0.3584708485721173,\n",
       " 0.032494125943784744,\n",
       " 0.032494125943784744,\n",
       " 0.5129091418912692,\n",
       " 0.5129091418912692,\n",
       " 0.7774982160898161,\n",
       " 0.7774982160898161,\n",
       " 0.729350459883324,\n",
       " 0.729350459883324]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1].edge_attr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
